{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import binned_statistic, binned_statistic_2d\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import patheffects\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dir\": \"../../outputs/librarian=S2\",\n",
    "    \"figure_dir\": \"../../analysis_data/figures\",\n",
    "    \"bins\": 16,\n",
    "    \"min_bin_percentile\": 1,\n",
    "    \"min_bin_percentile_for_cpy\": 10,\n",
    "    \"min_diff\": 2,\n",
    "    \"left_density\": \"SciBERT\",\n",
    "    \"right_density\": \"Word2Vec\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_field = {\n",
    "    'Bacon2019': 'Philosophy',\n",
    "    'hafenLowredshiftLymanLimit2017': 'Physics',\n",
    "    'Imeletal2022': 'Linguistics',\n",
    "    'Ololube2012': 'Education',\n",
    "    'Torres2013': 'Medicine',\n",
    "    'West2003': 'Economics',\n",
    "    'Miele2022': 'Materials Science',\n",
    "    'ForeroOrtega2021': 'Geology',\n",
    "    'Andre2018': 'Mathematics',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we don't have any old dataframes\n",
    "try:\n",
    "    del df_combined\n",
    "except NameError:\n",
    "    pass\n",
    "# Main loop\n",
    "for i, vectorizer_dir in enumerate(os.listdir(config[\"dir\"])):\n",
    "\n",
    "    # Combine same vectorizer, different centers\n",
    "    dfs = []\n",
    "    for j, center_dir in enumerate(\n",
    "        os.listdir(os.path.join(config[\"dir\"], vectorizer_dir))\n",
    "    ):\n",
    "        # Open\n",
    "        fp = os.path.join(config[\"dir\"], vectorizer_dir, center_dir, \"all_data.csv\")\n",
    "        df = pd.read_csv(fp)\n",
    "\n",
    "        # Skip dfs with no identifier\n",
    "        try:\n",
    "            df = df.set_index(\"identifier\")\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # Store center\n",
    "        center = center_dir.split(\"=\")[-1]\n",
    "        df[\"center\"] = center\n",
    "        df[\"field\"] = key_to_field[center]\n",
    "\n",
    "        # Add df to list\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Combine across centers\n",
    "    if len(dfs) == 0:\n",
    "        continue\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    # Rename columns\n",
    "    vectorizer = vectorizer_dir.split(\"=\")[-1]\n",
    "    for col in [\"density\", \"edginess\"]:\n",
    "        df[f\"{col}_{vectorizer}\"] = df[col]\n",
    "        del df[col]\n",
    "\n",
    "    # Combine across vectorizers\n",
    "    try:\n",
    "        df_combined = df_combined.join(\n",
    "            df.drop(\n",
    "                columns=[\n",
    "                    \"citations_per_year\",\n",
    "                    \"year\",\n",
    "                    \"references\",\n",
    "                    \"is_center\",\n",
    "                    \"center\",\n",
    "                    \"field\",\n",
    "                ]\n",
    "            ),\n",
    "            how=\"left\",\n",
    "        )\n",
    "    except NameError:\n",
    "        df_combined = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing data or duplicates\n",
    "df_combined = df_combined.dropna(\n",
    "    axis=\"index\",\n",
    "    how=\"any\",\n",
    "    subset=[f\"density_{config['left_density']}\", f\"density_{config['right_density']}\"],\n",
    ")\n",
    "df_combined = df_combined.loc[~df_combined.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentiles for density vars\n",
    "quantiles = np.linspace(0, 1, config[\"bins\"] + 1)\n",
    "density_vars = [var for var in df_combined.columns if \"density\" in var]\n",
    "density_bins = {}\n",
    "for var in density_vars:\n",
    "\n",
    "    # Get density\n",
    "    density = df_combined[var].values\n",
    "\n",
    "    # Get density bins\n",
    "    bins = np.linspace(\n",
    "        np.nanpercentile(density, config[\"min_bin_percentile\"]),\n",
    "        np.nanpercentile(density, 100 - config[\"min_bin_percentile\"]),\n",
    "        config[\"bins\"],\n",
    "    )\n",
    "    density_bins[var] = bins\n",
    "\n",
    "    # Calculate and use quantile bins\n",
    "    vectorizer = var.split(\"_\")[-1]\n",
    "    bins = np.quantile(density, quantiles)\n",
    "    df_combined.loc[:, f\"bin_{vectorizer}\"] = np.digitize(density, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: unmatched '[' (315026138.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    np.abs(df_combined[f\"bin_{config[\"left_density\"]}\"] -\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: unmatched '['\n"
     ]
    }
   ],
   "source": [
    "# Isolate entries where the density differs by >= min_diff bin\n",
    "df_combined[\"different_densities\"] = (\n",
    "        np.abs(df_combined[f\"bin_{config['left_density']}\"] -\n",
    "               df_combined[f\"bin_{config['right_density']}\"])\n",
    "    ) >= config[\"min_diff\"]\n",
    "df_combined[\"different_densities\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df_combined.groupby(\"field\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insufficient_data(x, ax):\n",
    "\n",
    "    if x.notna().sum() == 0:\n",
    "        ax.annotate(\n",
    "            text=f\"No data for {x.name}\",\n",
    "            xy=(0.5, 0.5),\n",
    "            xycoords=\"axes fraction\",\n",
    "            xytext=(0, 0),\n",
    "            textcoords=\"offset points\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "        )\n",
    "\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist(x, **kwargs):\n",
    "\n",
    "    if insufficient_data(x, plt.gca()):\n",
    "        return\n",
    "\n",
    "    bins = density_bins[x.name]\n",
    "    sns.histplot(x, bins=bins, **kwargs)\n",
    "    plt.xlim(bins[0], bins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist2d(x, y, **kwargs):\n",
    "\n",
    "    if insufficient_data(x, plt.gca()):\n",
    "        return\n",
    "    if insufficient_data(y, plt.gca()):\n",
    "        return\n",
    "\n",
    "    x_bins = density_bins[x.name]\n",
    "    y_bins = density_bins[y.name]\n",
    "    sns.histplot(x=x, y=y, bins=(x_bins, y_bins), **kwargs)\n",
    "    plt.xlim(x_bins[0], x_bins[-1])\n",
    "    plt.ylim(y_bins[0], y_bins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic visualization\n",
    "for field, group in grouped_df:\n",
    "\n",
    "    print(\"Distribution of density for\", field)\n",
    "\n",
    "    g = sns.PairGrid(group, vars=density_vars)\n",
    "    g.map_diag(hist)\n",
    "    g.map_offdiag(hist2d)\n",
    "    plt.suptitle(field, fontsize=20)\n",
    "    plt.show()\n",
    "    g.figure.savefig(\n",
    "        os.path.join(config[\"figure_dir\"], f\"density_per_vectorizer_for_{field}.png\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vs Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpy_bins = np.linspace(\n",
    "    np.nanpercentile(\n",
    "        df_combined[\"citations_per_year\"].dropna(), config[\"min_bin_percentile_for_cpy\"]\n",
    "    ),\n",
    "    np.nanpercentile(\n",
    "        df_combined[\"citations_per_year\"].dropna(),\n",
    "        100 - config[\"min_bin_percentile_for_cpy\"],\n",
    "    ),\n",
    "    config[\"bins\"],\n",
    ")\n",
    "cpy_median = df_combined[\"citations_per_year\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpy_vs_density(x, cpy, **kwargs):\n",
    "\n",
    "    if insufficient_data(x, plt.gca()):\n",
    "        return\n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    bins = density_bins[x.name]\n",
    "    centers = (bins[1:] + bins[:-1]) / 2\n",
    "\n",
    "    # Calculate running median\n",
    "    median, bin_edges, bin_number = binned_statistic(\n",
    "        x, cpy, statistic=\"median\", bins=bins\n",
    "    )\n",
    "    ax.plot(\n",
    "        centers,\n",
    "        median,\n",
    "        color=\"k\",\n",
    "    )\n",
    "\n",
    "    # Calculate running percentiles\n",
    "    low, bin_edges, bin_number = binned_statistic(\n",
    "        x,\n",
    "        cpy,\n",
    "        statistic=lambda v: np.nanpercentile(v, 16),\n",
    "        bins=bins,\n",
    "    )\n",
    "    high, bin_edges, bin_number = binned_statistic(\n",
    "        x,\n",
    "        cpy,\n",
    "        statistic=lambda v: np.nanpercentile(v, 84),\n",
    "        bins=bins,\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        centers,\n",
    "        low,\n",
    "        high,\n",
    "        color=\"k\",\n",
    "        alpha=0.1,\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(bins[0], bins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_cpy(x, y, cpy, **kwargs):\n",
    "\n",
    "    if insufficient_data(x, plt.gca()):\n",
    "        return\n",
    "    if insufficient_data(y, plt.gca()):\n",
    "        return\n",
    "\n",
    "    x_bins = density_bins[x.name]\n",
    "    y_bins = density_bins[y.name]\n",
    "\n",
    "    def _excess_above_median(z):\n",
    "\n",
    "        return (z > cpy.median()).sum() - z.size / 2\n",
    "\n",
    "    statistic, x_edges, y_edges, binnumber = binned_statistic_2d(\n",
    "        x,\n",
    "        y,\n",
    "        cpy,\n",
    "        bins=[x_bins, y_bins],\n",
    "        statistic=_excess_above_median,\n",
    "    )\n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.pcolormesh(\n",
    "        x_edges,\n",
    "        y_edges,\n",
    "        statistic.T,\n",
    "        shading=\"auto\",\n",
    "        vmin=-len(x) / 2 / config[\"bins\"] ** 2,\n",
    "        vmax=len(x) / 2 / config[\"bins\"] ** 2,\n",
    "        cmap=sns.color_palette(\"vlag\", as_cmap=True),\n",
    "        linewidth=0.0001,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(x_bins[0], x_bins[-1])\n",
    "    ax.set_ylim(y_bins[0], y_bins[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field, group in grouped_df:\n",
    "\n",
    "    print(\"Distribution of cpy vs density for\", field)\n",
    "\n",
    "    g = sns.PairGrid(group, vars=density_vars, corner=True)\n",
    "    g.map_diag(cpy_vs_density, cpy=group[\"citations_per_year\"])\n",
    "    g.map_offdiag(median_cpy, cpy=group[\"citations_per_year\"])\n",
    "    plt.suptitle(field, fontsize=20)\n",
    "    plt.show()\n",
    "    g.figure.savefig(\n",
    "        os.path.join(\n",
    "            config[\"figure_dir\"], f\"cpy_vs_density_per_vectorizer_for_{field}.png\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
