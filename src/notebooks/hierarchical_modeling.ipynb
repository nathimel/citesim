{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Modeling\n",
    "What models fit the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation imports\n",
    "import cmdstanpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.stats import norm, binned_statistic\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import sklearn.metrics as sk_metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"bins\": 16,\n",
    "    \"min_bin_percentile\": 10,\n",
    "    \"eval_sample_size\": 1000,\n",
    "    \"nonden_fit_cols\": [\"references\", \"year\"],\n",
    "    \"cat_col\": \"stan_field_code\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_ppd(log_p):\n",
    "    log_p_max = np.max(log_p)\n",
    "    log_sum_exp = log_p_max + np.log(np.sum(np.exp(log_p - log_p_max)))\n",
    "    return log_sum_exp - len(log_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"../../analysis_data/all_data.csv\")\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic nan handling across all obs\n",
    "df_all[\"log_cpy\"] = np.log10(df_all[\"citations_per_year\"])\n",
    "df_all[\"log_cpy\"] = df_all[\"log_cpy\"].replace(-np.inf, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the field category more useful\n",
    "df_all[\"fields_of_study_0\"] = df_all[\"fields_of_study_0\"].astype(\"category\")\n",
    "df_all[\"stan_field_code\"] = df_all[\"fields_of_study_0\"].cat.codes + 1\n",
    "field_names = df_all[\"fields_of_study_0\"].cat.categories\n",
    "field_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_names = sorted(df_all[\"vectorizer\"].unique())\n",
    "vectorizer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief aside on logscale plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.logspace(-3, 3, 100)\n",
    "log10 = np.log10(x)\n",
    "log10p = np.log10(1 + x)\n",
    "log10p2 = np.log10(1 + x) - 1\n",
    "\n",
    "ax.plot(\n",
    "    x,\n",
    "    log10,\n",
    "    label=\"log10\",\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    log10p,\n",
    "    label=\"log10p\",\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    log10p2,\n",
    "    label=\"log10p - 1\",\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview Normalized 2D Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bins(x, n_bins=config[\"bins\"]):\n",
    "    # Get density bins\n",
    "    bins = np.linspace(\n",
    "        np.nanpercentile(x, config[\"min_bin_percentile\"]),\n",
    "        np.nanpercentile(x, 100 - config[\"min_bin_percentile\"]),\n",
    "        n_bins + 1,\n",
    "    )\n",
    "    return bins\n",
    "\n",
    "\n",
    "density_bins = df_all.groupby(\"vectorizer\")[\"density\"].apply(get_bins).to_dict()\n",
    "cpy_bins = get_bins(\n",
    "    df_all[\"log_cpy\"], n_bins=17\n",
    ")  # The n_bins=17 is to ensure we don't accidentally flip axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_histplot(data, x, y, normed=True, *args, **kwargs):\n",
    "\n",
    "    # Get the appropriate facets\n",
    "    vectorizer = data[\"vectorizer\"].unique()[0]\n",
    "    field = data[\"fields_of_study_0\"].unique()[0]\n",
    "\n",
    "    density_bins = get_bins(data[x])\n",
    "\n",
    "    if normed:\n",
    "        hist2d, _, _ = np.histogram2d(data[x], data[y], bins=(density_bins, cpy_bins))\n",
    "        density_hist, _ = np.histogram(data[x], bins=density_bins)\n",
    "        hist2d_normed = hist2d / density_hist.reshape(1, -1).T\n",
    "\n",
    "        # Plot the data\n",
    "        plt.pcolormesh(\n",
    "            density_bins,\n",
    "            cpy_bins,\n",
    "            hist2d_normed.T,\n",
    "            shading=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "    else:\n",
    "        sns.histplot(data, x=x, y=y, bins=(density_bins, cpy_bins), *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = sns.FacetGrid(data=df_all, row=\"vectorizer\", col=\"fields_of_study_0\", sharex=False)\n",
    "fg.map_dataframe(custom_histplot, x=\"density\", y=\"log_cpy\", normed=True)\n",
    "fg.set_titles(\"{row_name} | {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focused Normalized Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.query(\"vectorizer in ['SciBERT', 'Word2Vec']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = sns.FacetGrid(data=df, row=\"vectorizer\", col=\"fields_of_study_0\", sharex=False)\n",
    "fg.map_dataframe(custom_histplot, x=\"density\", y=\"log_cpy\", normed=True)\n",
    "fg.set_titles(\"{row_name} | {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Modeling\n",
    "\n",
    "Note to future self: cmdstanpy and cmdstan advertise conda as *the* way to install.\n",
    "Don't listen to them.\n",
    "I couldn't get it to compile when I used conda for anything cmdstanpy related.\n",
    "I downloaded the repo and compiled manually, and used pip for cmdstanpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to wide format so we can use the same train_test division\n",
    "# and so we can use multiple densities at once\n",
    "df_vectorizers = df_all.pivot_table(\n",
    "    index=\"identifier\", columns=\"vectorizer\", values=\"density\"\n",
    ")\n",
    "# For the nonden cols we can take the first because the other values are duplicates\n",
    "nonden_cols = [\n",
    "    \"log_cpy\",\n",
    "    config[\"cat_col\"],\n",
    "] + config[\"nonden_fit_cols\"]\n",
    "df_others = df_all.groupby(\"identifier\")[nonden_cols].first()\n",
    "df_eval = pd.concat([df_others, df_vectorizers], axis=\"columns\")\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns where log_cpy is na\n",
    "df_eval = df_eval.dropna(subset=\"log_cpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaling (normalization by mean and sigma) to help with modelling\n",
    "scaled_cols = (\n",
    "    [\n",
    "        \"log_cpy\",\n",
    "    ]\n",
    "    + config[\"nonden_fit_cols\"]\n",
    "    + vectorizer_names\n",
    ")\n",
    "df_eval.loc[:, scaled_cols] = df_eval[scaled_cols].apply(scale)\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample\n",
    "if config[\"eval_sample_size\"] is not None:\n",
    "    df_eval = df_eval.sample(config[\"eval_sample_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "df_train, df_test = train_test_split(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store results in\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model\n",
    "The base model is just a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for storing output\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df_train_i = df_train.dropna(subset=[\"log_cpy\"])\n",
    "df_test_i = df_train.dropna(subset=[\"log_cpy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "data = {\n",
    "    \"N\": len(df_train_i),\n",
    "    \"y\": df_train_i[\"log_cpy\"].values,\n",
    "    \"N_test\": len(df_test_i),\n",
    "    \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "fit = model.sample(\n",
    "    data=data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse output\n",
    "draws = fit.draws_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store parameters\n",
    "output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "output[\"sigma\"] = draws[\"sigma\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log posterior predictive density\n",
    "output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[stan_model] = pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression on density\n",
    "$(c \\sim \\rho_t)$ vs $(c \\sim \\rho_v)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"reg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold modeling output\n",
    "\n",
    "# Loop through vectorizers\n",
    "outputs_for_this_model = {}\n",
    "for i, vectorizer_i in enumerate(vectorizer_names):\n",
    "    output = {}\n",
    "\n",
    "    # Drop na\n",
    "    df_train_i = df_train.dropna(subset=[vectorizer_i, \"log_cpy\"])\n",
    "    df_test_i = df_train.dropna(subset=[vectorizer_i, \"log_cpy\"])\n",
    "\n",
    "    # Format data\n",
    "    data = {\n",
    "        \"N\": len(df_train_i),\n",
    "        \"x\": df_train_i[vectorizer_i].values,\n",
    "        \"y\": df_train_i[\"log_cpy\"].values,\n",
    "        \"N_test\": len(df_test_i),\n",
    "        \"x_test\": df_test_i[vectorizer_i].values,\n",
    "        \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "    }\n",
    "\n",
    "    # Compile model\n",
    "    model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "    # Fit model\n",
    "    fit = model.sample(\n",
    "        data=data,\n",
    "    )\n",
    "\n",
    "    # Parse output\n",
    "    draws = fit.draws_pd()\n",
    "\n",
    "    # Store parameters\n",
    "    output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "    output[f\"beta[{vectorizer_i}]\"] = draws[\"beta\"].median()\n",
    "    output[\"sigma\"] = draws[\"sigma\"].median()\n",
    "\n",
    "    # Calculate log posterior predictive density\n",
    "    output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])\n",
    "\n",
    "    key = f\"{stan_model}_{vectorizer_i}\"\n",
    "    outputs_for_this_model[key] = output\n",
    "    results_dict[key] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for showing here\n",
    "output = pd.DataFrame(outputs_for_this_model).T\n",
    "output[\"vectorizer\"] = vectorizer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medianplot(data, x, y, bins, ax):\n",
    "\n",
    "    x = data[x]\n",
    "    y = data[y]\n",
    "\n",
    "    centers = (bins[1:] + bins[:-1]) / 2\n",
    "\n",
    "    # Calculate running median\n",
    "    median, bin_edges, bin_number = binned_statistic(\n",
    "        x, y, statistic=np.nanmedian, bins=bins\n",
    "    )\n",
    "    ax.plot(\n",
    "        centers,\n",
    "        median,\n",
    "        color=\"k\",\n",
    "    )\n",
    "\n",
    "    # Calculate running percentiles\n",
    "    low, bin_edges, bin_number = binned_statistic(\n",
    "        x,\n",
    "        y,\n",
    "        statistic=lambda v: np.nanpercentile(v, 16),\n",
    "        bins=bins,\n",
    "    )\n",
    "    high, bin_edges, bin_number = binned_statistic(\n",
    "        x,\n",
    "        y,\n",
    "        statistic=lambda v: np.nanpercentile(v, 84),\n",
    "        bins=bins,\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        centers,\n",
    "        low,\n",
    "        high,\n",
    "        color=\"k\",\n",
    "        alpha=0.1,\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(bins[0], bins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_edge = np.nanpercentile(\n",
    "    df_eval[vectorizer_names].values.flatten(), config[\"min_bin_percentile\"]\n",
    ")\n",
    "right_edge = -left_edge\n",
    "bins = np.linspace(left_edge, right_edge, config[\"bins\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in output.iterrows():\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    df_plot = df_eval.dropna(subset=[row[\"vectorizer\"], \"log_cpy\"])\n",
    "\n",
    "    medianplot(\n",
    "        data=df_plot,\n",
    "        x=row[\"vectorizer\"],\n",
    "        y=\"log_cpy\",\n",
    "        bins=bins,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Plot the regression\n",
    "    xs = bins\n",
    "    ys = row[\"alpha\"] + row[f\"beta[{row[\"vectorizer\"]}]\"] * xs\n",
    "    ax.plot(\n",
    "        xs,\n",
    "        ys,\n",
    "        color=palette[0],\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        xs,\n",
    "        ys - row[\"sigma\"],\n",
    "        ys + row[\"sigma\"],\n",
    "        color=palette[0],\n",
    "        alpha=0.4,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(f\"density [{row[\"vectorizer\"]}]\")\n",
    "    ax.set_ylabel(\"log_cpy\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression w/o density\n",
    "The next model is a multivate linear regression with no density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"multireg\"\n",
    "fit_cols = config[\"nonden_fit_cols\"]\n",
    "cols = [\n",
    "    \"log_cpy\",\n",
    "] + fit_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for storing output\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df_train_i = df_train[cols].dropna()\n",
    "df_test_i = df_train[cols].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "data = {\n",
    "    \"N\": len(df_train_i),\n",
    "    \"K\": len(fit_cols),\n",
    "    \"x\": df_train_i[fit_cols].values,\n",
    "    \"y\": df_train_i[\"log_cpy\"].values,\n",
    "    \"N_test\": len(df_test_i),\n",
    "    \"x_test\": df_test_i[fit_cols].values,\n",
    "    \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "fit = model.sample(\n",
    "    data=data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse output\n",
    "draws = fit.draws_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store parameters\n",
    "output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "output[\"sigma\"] = draws[\"sigma\"].median()\n",
    "beta_cols = [col for col in draws.columns if col[:4] == \"beta\"]\n",
    "betas = draws[beta_cols].median(axis=\"rows\")\n",
    "for i, fit_col in enumerate(fit_cols):\n",
    "    output[f\"beta[{fit_col}]\"] = betas.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log posterior predictive density\n",
    "output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[stan_model] = pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression\n",
    "Now with density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"multireg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold modeling output\n",
    "\n",
    "# Loop through vectorizers\n",
    "outputs_for_this_model = {}\n",
    "for i, vectorizer_i in enumerate(vectorizer_names):\n",
    "    output = {}\n",
    "\n",
    "    # Drop na\n",
    "    fit_cols = [\n",
    "        vectorizer_i,\n",
    "    ] + config[\"nonden_fit_cols\"]\n",
    "    cols = [\n",
    "        \"log_cpy\",\n",
    "    ] + fit_cols\n",
    "    df_train_i = df_train.dropna(subset=cols)\n",
    "    df_test_i = df_train.dropna(subset=cols)\n",
    "\n",
    "    # Format data\n",
    "    data = {\n",
    "        \"N\": len(df_train_i),\n",
    "        \"K\": len(fit_cols),\n",
    "        \"x\": df_train_i[fit_cols].values,\n",
    "        \"y\": df_train_i[\"log_cpy\"].values,\n",
    "        \"N_test\": len(df_test_i),\n",
    "        \"x_test\": df_test_i[fit_cols].values,\n",
    "        \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "    }\n",
    "\n",
    "    # Compile model\n",
    "    model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "    # Fit model\n",
    "    fit = model.sample(\n",
    "        data=data,\n",
    "    )\n",
    "\n",
    "    # Parse output\n",
    "    draws = fit.draws_pd()\n",
    "\n",
    "    # Store parameters\n",
    "    output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "    output[\"sigma\"] = draws[\"sigma\"].median()\n",
    "    beta_cols = [col for col in draws.columns if col[:4] == \"beta\"]\n",
    "    betas = draws[beta_cols].median(axis=\"rows\")\n",
    "    for i, fit_col in enumerate(fit_cols):\n",
    "        output[f\"beta[{fit_col}]\"] = betas.iloc[i]\n",
    "\n",
    "    # Calculate log posterior predictive density\n",
    "    output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])\n",
    "\n",
    "    key = f\"{stan_model}_{vectorizer_i}\"\n",
    "    outputs_for_this_model[key] = output\n",
    "    results_dict[key] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression w/ all fields\n",
    "The next model is a multivate linear regression using every variable we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"multireg\"\n",
    "fit_cols = config[\"nonden_fit_cols\"] + vectorizer_names\n",
    "cols = [\n",
    "    \"log_cpy\",\n",
    "] + fit_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for storing output\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df_train_i = df_train[cols].dropna()\n",
    "df_test_i = df_train[cols].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "data = {\n",
    "    \"N\": len(df_train_i),\n",
    "    \"K\": len(fit_cols),\n",
    "    \"x\": df_train_i[fit_cols].values,\n",
    "    \"y\": df_train_i[\"log_cpy\"].values,\n",
    "    \"N_test\": len(df_test_i),\n",
    "    \"x_test\": df_test_i[fit_cols].values,\n",
    "    \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "fit = model.sample(\n",
    "    data=data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse output\n",
    "draws = fit.draws_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store parameters\n",
    "output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "output[\"sigma\"] = draws[\"sigma\"].median()\n",
    "beta_cols = [col for col in draws.columns if col[:4] == \"beta\"]\n",
    "betas = draws[beta_cols].median(axis=\"rows\")\n",
    "for i, fit_col in enumerate(fit_cols):\n",
    "    output[f\"beta[{fit_col}]\"] = betas.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log posterior predictive density\n",
    "output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[f\"full_{stan_model}\"] = pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Linear Regression on density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"hreg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold modeling output\n",
    "\n",
    "# Loop through vectorizers\n",
    "outputs_for_this_model = {}\n",
    "for i, vectorizer_i in enumerate(vectorizer_names):\n",
    "    output = {}\n",
    "\n",
    "    # Drop na\n",
    "    df_train_i = df_train.dropna(subset=[vectorizer_i, \"log_cpy\"])\n",
    "    df_test_i = df_train.dropna(subset=[vectorizer_i, \"log_cpy\"])\n",
    "\n",
    "    # Format data\n",
    "    data = {\n",
    "        \"D\": 1,\n",
    "        \"N\": len(df_train_i),\n",
    "        \"L\": df_train_i[config[\"cat_col\"]].max(),\n",
    "        \"y\": df_train_i[\"log_cpy\"].values,\n",
    "        \"ll\": df_train_i[config[\"cat_col\"]].values,\n",
    "        \"x\": df_train_i[\n",
    "            [\n",
    "                vectorizer_i,\n",
    "            ]\n",
    "        ].values,\n",
    "    }\n",
    "\n",
    "    # Compile model\n",
    "    model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "    # Fit model\n",
    "    fit = model.sample(\n",
    "        data=data,\n",
    "    )\n",
    "\n",
    "    # Parse output\n",
    "    stan_vars = fit.stan_variables()\n",
    "\n",
    "    # Store parameters\n",
    "    output[\"alpha\"] = np.median(stan_vars[\"alpha\"])\n",
    "    output[\"sigma\"] = np.median(stan_vars[\"sigma\"])\n",
    "\n",
    "    # Store per-field betas\n",
    "    betas = np.nanmedian(stan_vars[\"beta\"], axis=0)\n",
    "    for i, betas_i in enumerate(betas):\n",
    "        field_name_i = field_names[i]\n",
    "        beta_i = betas_i[0]\n",
    "        output[f\"beta[{vectorizer_i},{field_name_i}]\"] = beta_i\n",
    "\n",
    "    # Calculate log posterior predictive density\n",
    "    # output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])\n",
    "\n",
    "    key = f\"{stan_model}_{vectorizer_i}\"\n",
    "    outputs_for_this_model[key] = output\n",
    "    results_dict[key] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for showing here\n",
    "output = pd.DataFrame(outputs_for_this_model).T\n",
    "output[\"vectorizer\"] = vectorizer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.4 * len(field_names), 4.8 * len(vectorizer_names)))\n",
    "mosaic = [[f\"{vectorizer_i}_{field_j}\" for field_j in field_names] for vectorizer_i in vectorizer_names]\n",
    "ax_dict = fig.subplot_mosaic(mosaic=mosaic)\n",
    "for i, (model_i, row) in enumerate(output.iterrows()):\n",
    "    vectorizer_i = row[\"vectorizer\"]\n",
    "    for j, field_j in enumerate(field_names):\n",
    "\n",
    "        ax_key = f\"{vectorizer_i}_{field_j}\" \n",
    "        ax = ax_dict[ax_key]\n",
    "\n",
    "        df_plot = df_eval.query(f\"stan_field_code == {j+1}\")\n",
    "\n",
    "        medianplot(\n",
    "            data=df_plot,\n",
    "            x=row[\"vectorizer\"],\n",
    "            y=\"log_cpy\",\n",
    "            bins=bins,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        # Plot the regression\n",
    "        xs = bins\n",
    "        ys = row[\"alpha\"] + row[f\"beta[{row[\"vectorizer\"]},{field_j}]\"] * xs\n",
    "        ax.plot(\n",
    "            xs,\n",
    "            ys,\n",
    "            color=palette[0],\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            xs,\n",
    "            ys - row[\"sigma\"],\n",
    "            ys + row[\"sigma\"],\n",
    "            color=palette[0],\n",
    "            alpha=0.4,\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(f\"density [{row[\"vectorizer\"]},{field_j}]\")\n",
    "        ax.set_ylabel(\"log_cpy\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results_dict).T\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Posterior Predictive Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.arange(len(results))\n",
    "ax.scatter(\n",
    "    x,\n",
    "    results[\"log_ppd\"],\n",
    ")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results.index, rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_cols = [col for col in results.columns if col[:4] == \"beta\"]\n",
    "betas = results[beta_cols].fillna(0.0)\n",
    "x_cols = [col[5:-1] for col in beta_cols]\n",
    "x_test = df_test[x_cols].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate y_pred\n",
    "y_pred = np.matmul(x_test.values, betas.T)\n",
    "y_pred += results[\"alpha\"].values.reshape(1, -1)\n",
    "y_pred.index = df_test.index\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte rmse\n",
    "results[\"rmse\"] = y_pred.apply(lambda x: root_mean_squared_error(df_test[\"log_cpy\"], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.arange(len(results))\n",
    "ax.scatter(\n",
    "    x,\n",
    "    results[\"rmse\"],\n",
    ")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results.index, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"rmse\")\n",
    "\n",
    "ax.set_ylim(0, ax.get_ylim()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_betas = results[beta_cols].max(axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.arange(len(results))\n",
    "ax.scatter(\n",
    "    x,\n",
    "    max_betas,\n",
    ")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results.index, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(r\"max($\\beta_i$)\")\n",
    "\n",
    "ax.set_ylim(0, ax.get_ylim()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max slopes correlation with rmse\n",
    "ax = sns.scatterplot(\n",
    "    results,\n",
    "    x=max_betas,\n",
    "    y=1 - results[\"rmse\"],\n",
    ")\n",
    "ax.set_xlim(0, ax.get_xlim()[1])\n",
    "ax.set_ylim(0, ax.get_ylim()[1])\n",
    "\n",
    "ax.set_xlabel(r\"$\\max(\\beta_i)$\")\n",
    "ax.set_ylabel(r\"1 - rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE vs distance from center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect rmse as a function of distance from the center\n",
    "for model_i in results.index:\n",
    "\n",
    "    row = results.loc[model_i]\n",
    "\n",
    "    # Only do models that have a density dependence\n",
    "    try:\n",
    "        vectorizer_i = model_i.split(\"_\")[-1]\n",
    "        assert vectorizer_i in vectorizer_names\n",
    "    except AssertionError:\n",
    "        continue\n",
    "\n",
    "    # Bin and labels setup\n",
    "    bins = np.linspace(-3, 3, config[\"bins\"])\n",
    "    centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    den_key = f\"rho_{vectorizer_i}\"\n",
    "    cut_key = f\"{den_key}_bin\"\n",
    "    model_key = f\"y_pred_{model_i}\"\n",
    "\n",
    "    # Format data into a new df and group it\n",
    "    df_test_to_group = pd.DataFrame()\n",
    "    df_test_to_group[den_key] = df_test[vectorizer_i]\n",
    "    df_test_to_group[model_key] = y_pred[model_i]\n",
    "    df_test_to_group[cut_key] = pd.cut(df_test_to_group[den_key], bins)\n",
    "    df_test_to_group[\"y_test\"] = df_test[\"log_cpy\"]\n",
    "    df_grouped = df_test_to_group.groupby(cut_key)\n",
    "\n",
    "    # Get mean predicted y per bin\n",
    "    def get_mean_y_pred(df):\n",
    "        if len(df) == 0:\n",
    "            return np.nan\n",
    "        return df[model_key].mean()\n",
    "\n",
    "    mean_y_pred = df_grouped.apply(get_mean_y_pred)\n",
    "\n",
    "    # Get rmse\n",
    "    def get_rmse_of_df(df):\n",
    "        if len(df) == 0:\n",
    "            return np.nan\n",
    "        return root_mean_squared_error(df[\"y_test\"], df[model_key])\n",
    "\n",
    "    rmse_per_bin = df_grouped.apply(get_rmse_of_df)\n",
    "\n",
    "    # Also get rmse if y_pred was just the mean\n",
    "    def get_rmse_from_mean(df):\n",
    "        if len(df) == 0:\n",
    "            return np.nan\n",
    "        y_pred = np.zeros(len(df))  # Since we normalized to 0\n",
    "        return root_mean_squared_error(df[\"y_test\"], y_pred)\n",
    "\n",
    "    rmse_from_mean = df_grouped.apply(get_rmse_from_mean)\n",
    "\n",
    "    # And rmse expected if it was a straight improvement on the mean that scales with\n",
    "    # slope\n",
    "    min_rmse = rmse_from_mean - np.abs(centers * row[f\"beta[{vectorizer_i}]\"])\n",
    "\n",
    "    # Plot\n",
    "    fig, ax_dict = plt.subplot_mosaic(\n",
    "        [\n",
    "            [\n",
    "                \"hist\",\n",
    "            ],\n",
    "            [\n",
    "                \"reg\",\n",
    "            ],\n",
    "            [\n",
    "                \"reg\",\n",
    "            ],\n",
    "            [\n",
    "                \"rmse\",\n",
    "            ],\n",
    "            [\n",
    "                \"rmse\",\n",
    "            ],\n",
    "        ],\n",
    "        figsize=(8, 12),\n",
    "    )\n",
    "\n",
    "    # Hist plot\n",
    "    ax = ax_dict[\"hist\"]\n",
    "    sns.histplot(\n",
    "        x=df_test_to_group[den_key],\n",
    "        ax=ax,\n",
    "        bins=bins,\n",
    "    )\n",
    "    ax.set_xlim(bins[0], bins[-1])\n",
    "    ax.set_title(model_i)\n",
    "    ax.set_xlabel(None)\n",
    "\n",
    "    # Regression plot\n",
    "    ax = ax_dict[\"reg\"]\n",
    "\n",
    "    # Plot the median\n",
    "    medianplot(\n",
    "        data=df_test_to_group,\n",
    "        x=den_key,\n",
    "        y=\"y_test\",\n",
    "        bins=bins,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Plot the regression\n",
    "    xs = bins\n",
    "    ys = row[\"alpha\"] + row[f\"beta[{vectorizer_i}]\"] * xs\n",
    "    ax.plot(\n",
    "        xs,\n",
    "        ys,\n",
    "        color=palette[0],\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        xs,\n",
    "        ys - row[\"sigma\"],\n",
    "        ys + row[\"sigma\"],\n",
    "        color=palette[0],\n",
    "        alpha=0.25,\n",
    "    )\n",
    "\n",
    "    # Plot what y_pred actually is\n",
    "    ax = sns.scatterplot(\n",
    "        x=centers,\n",
    "        y=mean_y_pred,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(None)\n",
    "    ax.set_ylabel(\"log_cpy\")\n",
    "\n",
    "    # RMSE plot\n",
    "    ax = ax_dict[\"rmse\"]\n",
    "    ax = sns.scatterplot(\n",
    "        x=centers,\n",
    "        y=rmse_per_bin,\n",
    "        ax=ax,\n",
    "        label=\"rmse\",\n",
    "    )\n",
    "    ax = sns.scatterplot(x=centers, y=rmse_from_mean, ax=ax, label=\"rmse(y_pred=0)\")\n",
    "    ax = sns.scatterplot(x=centers, y=min_rmse, ax=ax, label=\"min rmse\")\n",
    "    ax.legend()\n",
    "    # Center line\n",
    "    ax.axvline(\n",
    "        0,\n",
    "        c=\"0.2\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    # Global rmse line\n",
    "    ax.axhline(\n",
    "        results.loc[model_i, \"rmse\"],\n",
    "        c=\"0.2\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    ax.set_ylabel(\"rmse\")\n",
    "    ax.set_xlabel(den_key)\n",
    "\n",
    "    ax.set_xlim(bins[0], bins[-1])\n",
    "    ax.set_ylim(0, ax.get_ylim()[1])\n",
    "    fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that RMSE is capturing two behaviors: distance from the mean and also width of the distribution.\n",
    "The width of the distribution largely dominates over the distance from the mean.\n",
    "So instead we may want a metric that is an evaluation of the distance median.\n",
    "This might relate to Nathaniel's choice to bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    r\"$R^2$\": sk_metrics.r2_score,\n",
    "    \"1-MAE\": lambda y_true, y_pred: 1 - sk_metrics.mean_absolute_error(y_true, y_pred),\n",
    "    \"1-RMSE\": lambda y_true, y_pred: 1\n",
    "    - sk_metrics.root_mean_squared_error(y_true, y_pred),\n",
    "    \"1-MAPE\": lambda y_true, y_pred: 1\n",
    "    - sk_metrics.mean_absolute_percentage_error(y_true, y_pred),\n",
    "    \"1-MedAE\": lambda y_true, y_pred: 1\n",
    "    - sk_metrics.median_absolute_error(y_true, y_pred),\n",
    "    \"1-MPinL\": lambda y_true, y_pred: 1 - sk_metrics.mean_pinball_loss(y_true, y_pred),\n",
    "    r\"$D^2_{pin}$\": sk_metrics.d2_pinball_score,\n",
    "    r\"$D^2_{abs}$\": sk_metrics.d2_absolute_error_score,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_label, metric_fn in metrics.items():\n",
    "\n",
    "    metric = y_pred.apply(lambda x: metric_fn(df_test[\"log_cpy\"], x))\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    x = np.arange(len(results))\n",
    "    ax.scatter(\n",
    "        x,\n",
    "        metric,\n",
    "    )\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(results.index, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(metric_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citesim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
