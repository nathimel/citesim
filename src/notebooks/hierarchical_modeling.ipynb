{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Modeling\n",
    "What models fit the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation imports\n",
    "import cmdstanpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.stats import norm, binned_statistic\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"bins\": 16,\n",
    "    \"min_bin_percentile\": 10,\n",
    "    \"eval_sample_size\": 1000,\n",
    "    \"nonden_fit_cols\": [\"references\", \"year\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_ppd(log_p):\n",
    "    log_p_max = np.max(log_p)\n",
    "    log_sum_exp = log_p_max + np.log(np.sum(np.exp(log_p - log_p_max)))\n",
    "    return log_sum_exp - len(log_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"../../analysis_data/all_data.csv\")\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic nan handling across all obs\n",
    "df_all[\"log_cpy\"] = np.log10(df_all[\"citations_per_year\"])\n",
    "df_all[\"log_cpy\"] = df_all[\"log_cpy\"].replace(-np.inf, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = sorted(df_all[\"fields_of_study_0\"].unique())\n",
    "field_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_names = sorted(df_all[\"vectorizer\"].unique())\n",
    "vectorizer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief aside on logscale plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.logspace(-3, 3, 100)\n",
    "log10 = np.log10(x)\n",
    "log10p = np.log10(1 + x)\n",
    "log10p2 = np.log10(1 + x) - 1\n",
    "\n",
    "ax.plot(\n",
    "    x,\n",
    "    log10,\n",
    "    label=\"log10\",\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    log10p,\n",
    "    label=\"log10p\",\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    log10p2,\n",
    "    label=\"log10p - 1\",\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview Normalized 2D Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bins(x, n_bins=config[\"bins\"]):\n",
    "    # Get density bins\n",
    "    bins = np.linspace(\n",
    "        np.nanpercentile(x, config[\"min_bin_percentile\"]),\n",
    "        np.nanpercentile(x, 100 - config[\"min_bin_percentile\"]),\n",
    "        n_bins + 1,\n",
    "    )\n",
    "    return bins\n",
    "\n",
    "\n",
    "density_bins = df_all.groupby(\"vectorizer\")[\"density\"].apply(get_bins).to_dict()\n",
    "cpy_bins = get_bins(\n",
    "    df_all[\"log_cpy\"], n_bins=17\n",
    ")  # The n_bins=17 is to ensure we don't accidentally flip axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_histplot(data, x, y, normed=True, *args, **kwargs):\n",
    "\n",
    "    # Get the appropriate facets\n",
    "    vectorizer = data[\"vectorizer\"].unique()[0]\n",
    "    field = data[\"fields_of_study_0\"].unique()[0]\n",
    "\n",
    "    density_bins = get_bins(data[x])\n",
    "\n",
    "    if normed:\n",
    "        hist2d, _, _ = np.histogram2d(data[x], data[y], bins=(density_bins, cpy_bins))\n",
    "        density_hist, _ = np.histogram(data[x], bins=density_bins)\n",
    "        hist2d_normed = hist2d / density_hist.reshape(1, -1).T\n",
    "\n",
    "        # Plot the data\n",
    "        plt.pcolormesh(\n",
    "            density_bins,\n",
    "            cpy_bins,\n",
    "            hist2d_normed.T,\n",
    "            shading=\"auto\",\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "    else:\n",
    "        sns.histplot(data, x=x, y=y, bins=(density_bins, cpy_bins), *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = sns.FacetGrid(data=df_all, row=\"vectorizer\", col=\"fields_of_study_0\", sharex=False)\n",
    "fg.map_dataframe(custom_histplot, x=\"density\", y=\"log_cpy\", normed=True)\n",
    "fg.set_titles(\"{row_name} | {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focused Normalized Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.query(\"vectorizer in ['SciBERT', 'Word2Vec']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = sns.FacetGrid(data=df, row=\"vectorizer\", col=\"fields_of_study_0\", sharex=False)\n",
    "fg.map_dataframe(custom_histplot, x=\"density\", y=\"log_cpy\", normed=True)\n",
    "fg.set_titles(\"{row_name} | {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Modeling\n",
    "\n",
    "Note to future self: cmdstanpy and cmdstan advertise conda as *the* way to install.\n",
    "Don't listen to them.\n",
    "I couldn't get it to compile when I used conda for anything cmdstanpy related.\n",
    "I downloaded the repo and compiled manually, and used pip for cmdstanpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to wide format so we can use the same train_test division\n",
    "# and so we can use multiple densities at once\n",
    "df_vectorizers = df_all.pivot_table(\n",
    "    index=\"identifier\", columns=\"vectorizer\", values=\"density\"\n",
    ")\n",
    "# For the nonden cols we can take the first because the other values are duplicates\n",
    "nonden_cols = [\"log_cpy\", ] + config[\"nonden_fit_cols\"]\n",
    "df_others = df_all.groupby(\"identifier\")[nonden_cols].first()\n",
    "df_eval = pd.concat([df_others, df_vectorizers], axis=\"columns\")\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns where log_cpy is na\n",
    "df_eval = df_eval.dropna(subset=\"log_cpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaling (normalization by mean and sigma) to help with modelling\n",
    "df_eval = df_eval.apply(scale)\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample\n",
    "if config[\"eval_sample_size\"] is not None:\n",
    "    df_eval = df_eval.sample(config[\"eval_sample_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "df_train, df_test = train_test_split(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store results in\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model\n",
    "The base model is just a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for storing output\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df_train_i = df_train.dropna(subset=[\"log_cpy\"])\n",
    "df_test_i = df_train.dropna(subset=[\"log_cpy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "data = {\n",
    "    \"N\": len(df_train_i),\n",
    "    \"y\": df_train_i[\"log_cpy\"].values,\n",
    "    \"N_test\": len(df_test_i),\n",
    "    \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "fit = model.sample(\n",
    "    data=data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse output\n",
    "draws = fit.draws_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store parameters\n",
    "output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "output[\"sigma\"] = draws[\"sigma\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log posterior predictive density\n",
    "output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[stan_model] = pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression on density\n",
    "$(c \\sim \\rho_t)$ vs $(c \\sim \\rho_v)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"reg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold modeling output\n",
    "\n",
    "# Loop through vectorizers\n",
    "outputs_for_this_model = {}\n",
    "for i, vectorizer_i in enumerate(vectorizer_names):\n",
    "    output = {}\n",
    "\n",
    "    # Drop na\n",
    "    df_train_i = df_train.dropna(subset=[vectorizer_i, \"log_cpy\"])\n",
    "    df_test_i = df_train.dropna(subset=[vectorizer_i, \"log_cpy\"])\n",
    "\n",
    "    # Format data\n",
    "    data = {\n",
    "        \"N\": len(df_train_i),\n",
    "        \"x\": df_train_i[vectorizer_i].values,\n",
    "        \"y\": df_train_i[\"log_cpy\"].values,\n",
    "        \"N_test\": len(df_test_i),\n",
    "        \"x_test\": df_test_i[vectorizer_i].values,\n",
    "        \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "    }\n",
    "\n",
    "    # Compile model\n",
    "    model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "    # Fit model\n",
    "    fit = model.sample(\n",
    "        data=data,\n",
    "    )\n",
    "\n",
    "    # Parse output\n",
    "    draws = fit.draws_pd()\n",
    "\n",
    "    # Store parameters\n",
    "    output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "    output[f\"beta[{vectorizer_i}]\"] = draws[\"beta\"].median()\n",
    "    output[\"sigma\"] = draws[\"sigma\"].median()\n",
    "\n",
    "    # Calculate log posterior predictive density\n",
    "    output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])\n",
    "\n",
    "    key = f\"{stan_model}_{vectorizer_i}\"\n",
    "    outputs_for_this_model[key] = output\n",
    "    results_dict[key] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for showing here\n",
    "output = pd.DataFrame(outputs_for_this_model).T\n",
    "output[\"vectorizer\"] = vectorizer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medianplot(data, x, y, bins, ax):\n",
    "\n",
    "    x = data[x]\n",
    "    y = data[y]\n",
    "\n",
    "    centers = (bins[1:] + bins[:-1]) / 2\n",
    "\n",
    "    # Calculate running median\n",
    "    median, bin_edges, bin_number = binned_statistic(\n",
    "        x, y, statistic=np.nanmedian, bins=bins\n",
    "    )\n",
    "    ax.plot(\n",
    "        centers,\n",
    "        median,\n",
    "        color=\"k\",\n",
    "    )\n",
    "\n",
    "    # Calculate running percentiles\n",
    "    low, bin_edges, bin_number = binned_statistic(\n",
    "        x,\n",
    "        y,\n",
    "        statistic=lambda v: np.nanpercentile(v, 16),\n",
    "        bins=bins,\n",
    "    )\n",
    "    high, bin_edges, bin_number = binned_statistic(\n",
    "        x,\n",
    "        y,\n",
    "        statistic=lambda v: np.nanpercentile(v, 84),\n",
    "        bins=bins,\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        centers,\n",
    "        low,\n",
    "        high,\n",
    "        color=\"k\",\n",
    "        alpha=0.1,\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(bins[0], bins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_edge = np.nanpercentile(\n",
    "    df_eval[vectorizer_names].values.flatten(), config[\"min_bin_percentile\"]\n",
    ")\n",
    "right_edge = -left_edge\n",
    "bins = np.linspace(left_edge, right_edge, config[\"bins\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in output.iterrows():\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    medianplot(\n",
    "        data=df_eval,\n",
    "        x=row[\"vectorizer\"],\n",
    "        y=\"log_cpy\",\n",
    "        bins=bins,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Plot the regression\n",
    "    xs = bins\n",
    "    ys = row[\"alpha\"] + row[f\"beta[{row[\"vectorizer\"]}]\"] * xs\n",
    "    ax.plot(\n",
    "        xs,\n",
    "        ys,\n",
    "        color=palette[0],\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        xs,\n",
    "        ys - row[\"sigma\"],\n",
    "        ys + row[\"sigma\"],\n",
    "        color=palette[0],\n",
    "        alpha=0.4,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(f\"density [{row[\"vectorizer\"]}]\")\n",
    "    ax.set_ylabel(\"log_cpy\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression w/o density\n",
    "The next model is a multivate linear regression with no density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"multireg\"\n",
    "fit_cols = config[\"nonden_fit_cols\"]\n",
    "cols = [\"log_cpy\", ] + fit_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for storing output\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df_train_i = df_train[cols].dropna()\n",
    "df_test_i = df_train[cols].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "data = {\n",
    "    \"N\": len(df_train_i),\n",
    "    \"K\": len(fit_cols),\n",
    "    \"x\": df_train_i[fit_cols].values,\n",
    "    \"y\": df_train_i[\"log_cpy\"].values,\n",
    "    \"N_test\": len(df_test_i),\n",
    "    \"x_test\": df_test_i[fit_cols].values,\n",
    "    \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "fit = model.sample(\n",
    "    data=data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse output\n",
    "draws = fit.draws_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store parameters\n",
    "output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "output[\"sigma\"] = draws[\"sigma\"].median()\n",
    "beta_cols = [col for col in draws.columns if col[:4] == \"beta\"]\n",
    "betas = draws[beta_cols].median(axis=\"rows\")\n",
    "for i, fit_col in enumerate(fit_cols):\n",
    "    output[f\"beta[{fit_col}]\"] = betas.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log posterior predictive density\n",
    "output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[stan_model] = pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression\n",
    "Now with density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"multireg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold modeling output\n",
    "\n",
    "# Loop through vectorizers\n",
    "outputs_for_this_model = {}\n",
    "for i, vectorizer_i in enumerate(vectorizer_names):\n",
    "    output = {}\n",
    "\n",
    "    # Drop na\n",
    "    fit_cols = [vectorizer_i, ] + config[\"nonden_fit_cols\"]\n",
    "    cols = [\"log_cpy\", ] + fit_cols\n",
    "    df_train_i = df_train.dropna(subset=cols)\n",
    "    df_test_i = df_train.dropna(subset=cols)\n",
    "\n",
    "    # Format data\n",
    "    data = {\n",
    "        \"N\": len(df_train_i),\n",
    "        \"K\": len(fit_cols),\n",
    "        \"x\": df_train_i[fit_cols].values,\n",
    "        \"y\": df_train_i[\"log_cpy\"].values,\n",
    "        \"N_test\": len(df_test_i),\n",
    "        \"x_test\": df_test_i[fit_cols].values,\n",
    "        \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "    }\n",
    "\n",
    "    # Compile model\n",
    "    model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "    # Fit model\n",
    "    fit = model.sample(\n",
    "        data=data,\n",
    "    )\n",
    "\n",
    "    # Parse output\n",
    "    draws = fit.draws_pd()\n",
    "\n",
    "    # Store parameters\n",
    "    output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "    output[\"sigma\"] = draws[\"sigma\"].median()\n",
    "    beta_cols = [col for col in draws.columns if col[:4] == \"beta\"]\n",
    "    betas = draws[beta_cols].median(axis=\"rows\")\n",
    "    for i, fit_col in enumerate(fit_cols):\n",
    "        output[f\"beta[{fit_col}]\"] = betas.iloc[i]\n",
    "\n",
    "    # Calculate log posterior predictive density\n",
    "    output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])\n",
    "\n",
    "    key = f\"{stan_model}_{vectorizer_i}\"\n",
    "    outputs_for_this_model[key] = output\n",
    "    results_dict[key] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate linear regression w/ all fields\n",
    "The next model is a multivate linear regression using every variable we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"multireg\"\n",
    "fit_cols = config[\"nonden_fit_cols\"] + vectorizer_names\n",
    "cols = [\"log_cpy\", ] + fit_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for storing output\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df_train_i = df_train[cols].dropna()\n",
    "df_test_i = df_train[cols].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "data = {\n",
    "    \"N\": len(df_train_i),\n",
    "    \"K\": len(fit_cols),\n",
    "    \"x\": df_train_i[fit_cols].values,\n",
    "    \"y\": df_train_i[\"log_cpy\"].values,\n",
    "    \"N_test\": len(df_test_i),\n",
    "    \"x_test\": df_test_i[fit_cols].values,\n",
    "    \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "fit = model.sample(\n",
    "    data=data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse output\n",
    "draws = fit.draws_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store parameters\n",
    "output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "output[\"sigma\"] = draws[\"sigma\"].median()\n",
    "beta_cols = [col for col in draws.columns if col[:4] == \"beta\"]\n",
    "betas = draws[beta_cols].median(axis=\"rows\")\n",
    "for i, fit_col in enumerate(fit_cols):\n",
    "    output[f\"beta[{fit_col}]\"] = betas.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log posterior predictive density\n",
    "output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[f\"full_{stan_model}\"] = pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results_dict).T\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Posterior Predictive Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.arange(len(results))\n",
    "ax.scatter(\n",
    "    x,\n",
    "    results[\"log_ppd\"],\n",
    ")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results.index, rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_cols = [col for col in results.columns if col[:4] == \"beta\"]\n",
    "betas = results[beta_cols].fillna(0.)\n",
    "x_cols = [col[5:-1] for col in beta_cols]\n",
    "x_test = df_test[x_cols].fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate y_pred\n",
    "y_pred = np.matmul(x_test.values, betas.T)\n",
    "y_pred += results[\"alpha\"].values.reshape(1, -1)\n",
    "y_pred.index = df_test.index\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte rmse\n",
    "results[\"rmse\"] = y_pred.apply(lambda x: root_mean_squared_error(df_test[\"log_cpy\"], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.arange(len(results))\n",
    "ax.scatter(\n",
    "    x,\n",
    "    results[\"rmse\"],\n",
    ")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results.index, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"rmse\")\n",
    "\n",
    "ax.set_ylim(0, ax.get_ylim()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_betas = results[beta_cols].max(axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.arange(len(results))\n",
    "ax.scatter(\n",
    "    x,\n",
    "    max_betas,\n",
    ")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results.index, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(r\"max($\\beta_i$)\")\n",
    "\n",
    "ax.set_ylim(0, ax.get_ylim()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max slopes correlation with rmse\n",
    "ax = sns.scatterplot(\n",
    "    results,\n",
    "    x=max_betas,\n",
    "    y=1 - results[\"rmse\"],\n",
    ")\n",
    "ax.set_xlim(0, ax.get_xlim()[1])\n",
    "ax.set_ylim(0, ax.get_ylim()[1])\n",
    "\n",
    "ax.set_xlabel(r'$\\max(\\beta_i)$')\n",
    "ax.set_ylabel(r'1 - rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect rmse as a function of distance from the center\n",
    "for model_i in results.index:\n",
    "\n",
    "    try:\n",
    "        vectorizer_i = model_i.split(\"_\")[-1]\n",
    "        assert vectorizer_i in vectorizer_names\n",
    "    except AssertionError:\n",
    "        continue\n",
    "\n",
    "    bins = np.linspace(-3, 3, config[\"bins\"])\n",
    "    den_key = f\"rho_{vectorizer_i}\"\n",
    "    cut_key = f\"{den_key}_bin\"\n",
    "    model_key = f\"y_pred_{model_i}\"\n",
    "    df_test_to_group = pd.DataFrame()\n",
    "    df_test_to_group[den_key] = df_test[vectorizer_i]\n",
    "    df_test_to_group[model_key] = y_pred[model_i]\n",
    "    df_test_to_group[cut_key] = pd.cut(df_test_to_group[den_key], bins)\n",
    "    df_test_to_group[\"y_test\"] = df_test[\"log_cpy\"]\n",
    "    def get_rmse_of_df(df):\n",
    "        if len(df) == 0:\n",
    "            return np.nan\n",
    "        return root_mean_squared_error(df[\"y_test\"], df[model_key])\n",
    "    rmse_per_bin = df_test_to_group.groupby(cut_key).apply(get_rmse_of_df)\n",
    "    centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax = sns.scatterplot(\n",
    "        x=centers,\n",
    "        y=rmse_per_bin,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.axvline(\n",
    "        0,\n",
    "        c='0.2',\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    ax.axhline(\n",
    "        results.loc[model_i, \"rmse\"],\n",
    "        c='0.2',\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    ax.set_ylabel(\"rmse\")\n",
    "    ax.set_xlabel(den_key)\n",
    "\n",
    "    ax.set_ylim(0, ax.get_ylim()[1])\n",
    "    fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citesim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
