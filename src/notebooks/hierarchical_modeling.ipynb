{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Modeling\n",
    "What models fit the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"bins\": 16,\n",
    "    \"min_bin_percentile\": 10,\n",
    "    \"eval_sample_size\": 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"../../analysis_data/all_data.csv\")\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic nan handling across all obs\n",
    "df_all[\"log_cpy\"] = np.log10(df_all[\"citations_per_year\"])\n",
    "df_all['log_cpy'] = df_all['log_cpy'].replace(-np.inf, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = sorted(df_all[\"fields_of_study_0\"].unique())\n",
    "field_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_names = sorted(df_all[\"vectorizer\"].unique())\n",
    "vectorizer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief aside on logscale plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.logspace(-3, 3, 100)\n",
    "log10 = np.log10(x)\n",
    "log10p = np.log10(1 + x)\n",
    "log10p2 = np.log10(1 + x) - 1\n",
    "\n",
    "ax.plot(\n",
    "    x,\n",
    "    log10,\n",
    "    label=\"log10\",\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    log10p,\n",
    "    label=\"log10p\",\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    log10p2,\n",
    "    label=\"log10p - 1\",\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview Normalized 2D Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bins(x, n_bins=config[\"bins\"]):\n",
    "    # Get density bins\n",
    "    bins = np.linspace(\n",
    "        np.nanpercentile(x, config[\"min_bin_percentile\"]),\n",
    "        np.nanpercentile(x, 100 - config[\"min_bin_percentile\"]),\n",
    "        n_bins + 1,\n",
    "    )\n",
    "    return bins\n",
    "\n",
    "\n",
    "density_bins = df_all.groupby(\"vectorizer\")[\"density\"].apply(get_bins).to_dict()\n",
    "cpy_bins = get_bins(\n",
    "    df_all[\"log_cpy\"], n_bins=17\n",
    ")  # The n_bins=17 is to ensure we don't accidentally flip axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_histplot(data, x, y, normed=True, *args, **kwargs):\n",
    "\n",
    "    # Get the appropriate facets\n",
    "    vectorizer = data[\"vectorizer\"].unique()[0]\n",
    "    field = data[\"fields_of_study_0\"].unique()[0]\n",
    "\n",
    "    density_bins = get_bins(data[x])\n",
    "\n",
    "    if normed:\n",
    "        hist2d, _, _ = np.histogram2d(data[x], data[y], bins=(density_bins, cpy_bins))\n",
    "        density_hist, _ = np.histogram(data[x], bins=density_bins)\n",
    "        hist2d_normed = hist2d / density_hist.reshape(1, -1).T\n",
    "\n",
    "        # Plot the data\n",
    "        plt.pcolormesh(\n",
    "            density_bins,\n",
    "            cpy_bins,\n",
    "            hist2d_normed.T,\n",
    "            shading='auto',\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "    else:\n",
    "        sns.histplot(data, x=x, y=y, bins=(density_bins, cpy_bins), *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = sns.FacetGrid(data=df_all, row=\"vectorizer\", col=\"fields_of_study_0\", sharex=False)\n",
    "fg.map_dataframe(custom_histplot, x=\"density\", y=\"log_cpy\", normed=True)\n",
    "fg.set_titles(\"{row_name} | {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focused Normalized Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.query(\"vectorizer in ['SciBERT', 'Word2Vec']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = sns.FacetGrid(data=df, row=\"vectorizer\", col=\"fields_of_study_0\", sharex=False)\n",
    "fg.map_dataframe(custom_histplot, x=\"density\", y=\"log_cpy\", normed=True)\n",
    "fg.set_titles(\"{row_name} | {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Modeling\n",
    "\n",
    "Note to future self: cmdstanpy and cmdstan advertise conda as *the* way to install.\n",
    "Don't listen to them.\n",
    "I couldn't get it to compile when I used conda for anything cmdstanpy related.\n",
    "I downloaded the repo and compiled manually, and used pip for cmdstanpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmdstanpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to wide format so we can use the same train_test division\n",
    "df_eval = df_all.pivot_table(index=\"identifier\", columns=\"vectorizer\", values=\"density\")\n",
    "vectorizers = df_eval.columns\n",
    "log_cpy = df_all.pivot_table(index=\"identifier\", columns=\"vectorizer\", values=\"log_cpy\").iloc[:, 0]\n",
    "df_eval[\"log_cpy\"] = log_cpy\n",
    "df_eval.columns.name = None\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaling (normalization by mean and sigma) to help with modelling\n",
    "df_eval = df_eval.apply(scale)\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample\n",
    "if config[\"eval_sample_size\"] is not None:\n",
    "    df_eval = df_eval.sample(config[\"eval_sample_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "df_train, df_test = train_test_split(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global fits\n",
    "$(c \\sim \\rho_t)$ vs $(c \\sim \\rho_v)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"globalreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold modeling output\n",
    "output = {\n",
    "    \"vectorizer\": [],\n",
    "    \"alpha\": [],\n",
    "    \"beta\": [],\n",
    "    \"sigma\": [],\n",
    "    \"rmse\": [],\n",
    "    \"log_ppd\": [],\n",
    "}\n",
    "\n",
    "# Loop through vectorizers\n",
    "for i, vectorizer_i in enumerate(vectorizers):\n",
    "\n",
    "    # Drop na\n",
    "    df_train_i = df_train.dropna(subset=[vectorizer_i, \"log_cpy\"])\n",
    "    df_test_i = df_train.dropna(subset=[vectorizer_i, \"log_cpy\"])\n",
    "\n",
    "    # Format data\n",
    "    data = {\n",
    "        \"N\": len(df_train_i),\n",
    "        \"x\": df_train_i[vectorizer_i].values,\n",
    "        \"y\": df_train_i[\"log_cpy\"].values,\n",
    "        \"N_test\": len(df_test_i),\n",
    "        \"x_test\": df_test_i[vectorizer_i].values,\n",
    "        \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "    }\n",
    "\n",
    "    # Compile model\n",
    "    model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "    # Fit model\n",
    "    fit = model.sample(\n",
    "        data=data,\n",
    "    )\n",
    "\n",
    "    # Parse output\n",
    "    draws = fit.draws_pd()\n",
    "    y_tilde_cols = [col for col in draws.columns if 'y_tilde' in col]\n",
    "    columns = [col for col in draws.columns if 'y_tilde' not in col]\n",
    "    y_pred_i = draws[y_tilde_cols].mean(axis=\"rows\").values\n",
    "\n",
    "    # Store label\n",
    "    output[\"vectorizer\"].append(vectorizer_i)\n",
    "\n",
    "    # Store parameters\n",
    "    output[\"alpha\"].append(draws[\"alpha\"].median())\n",
    "    output[\"beta\"].append(draws[\"beta\"].median())\n",
    "    output[\"sigma\"].append(draws[\"sigma\"].median())\n",
    "\n",
    "    # Calculate rmse\n",
    "    rmse_i = root_mean_squared_error(df_test_i[\"log_cpy\"], y_pred_i)\n",
    "    output[\"rmse\"].append(rmse_i)\n",
    "\n",
    "    # Calculate log posterior predictive density\n",
    "    log_ppd = scipy.special.logsumexp(draws[\"log_p\"]) - np.log(len(draws))\n",
    "    output[\"log_ppd\"].append(log_ppd)\n",
    "output = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medianplot(data, x, y, bins, ax):\n",
    "\n",
    "    x = data[x]\n",
    "    y = data[y]\n",
    "\n",
    "    centers = (bins[1:] + bins[:-1]) / 2\n",
    "\n",
    "    # Calculate running median\n",
    "    median, bin_edges, bin_number = binned_statistic(\n",
    "        x, y, statistic=np.nanmedian, bins=bins\n",
    "    )\n",
    "    ax.plot(\n",
    "        centers,\n",
    "        median,\n",
    "        color=\"k\",\n",
    "    )\n",
    "\n",
    "    # Calculate running percentiles\n",
    "    low, bin_edges, bin_number = binned_statistic(\n",
    "        x,\n",
    "        y,\n",
    "        statistic=lambda v: np.nanpercentile(v, 16),\n",
    "        bins=bins,\n",
    "    )\n",
    "    high, bin_edges, bin_number = binned_statistic(\n",
    "        x,\n",
    "        y,\n",
    "        statistic=lambda v: np.nanpercentile(v, 84),\n",
    "        bins=bins,\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        centers,\n",
    "        low,\n",
    "        high,\n",
    "        color=\"k\",\n",
    "        alpha=0.1,\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(bins[0], bins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_edge = np.nanpercentile(df_eval[vectorizers].values.flatten(), config[\"min_bin_percentile\"])\n",
    "right_edge = -left_edge\n",
    "bins = np.linspace(left_edge, right_edge, config[\"bins\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in output.iterrows():\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    medianplot(\n",
    "        data=df_eval,\n",
    "        x=row[\"vectorizer\"],\n",
    "        y=\"log_cpy\",\n",
    "        bins=bins,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Plot the regression\n",
    "    xs = bins\n",
    "    ys = row[\"alpha\"] + row[\"beta\"] * xs\n",
    "    ax.plot(\n",
    "        xs,\n",
    "        ys,\n",
    "        color=palette[0],\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        xs,\n",
    "        ys - row[\"sigma\"],\n",
    "        ys + row[\"sigma\"],\n",
    "        color=palette[0],\n",
    "        alpha=0.4,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(f\"density [{row[\"vectorizer\"]}]\")\n",
    "    ax.set_ylabel(\"log_cpy\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citesim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
