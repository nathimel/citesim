{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Modeling\n",
    "What models fit the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do:\n",
    "\n",
    "### In Progress\n",
    "\n",
    "- show y_pred as part of the median plot\n",
    "- implement model selection or feature selection\n",
    "\n",
    "### On Hold\n",
    "- have median plot alpha scale with hist (requires LineCollection)\n",
    "- caching (will require me to implement something)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stdlib imports\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation imports\n",
    "import cmdstanpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy.stats import norm, binned_statistic\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import sklearn.metrics as sk_metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"bins\": 16,\n",
    "    \"min_bin_percentile\": 10,\n",
    "    \"eval_sample_size\": 100,\n",
    "    \"nonden_fit_cols\": [\"references\", \"year\"],\n",
    "    \"cat_col\": \"stan_field_code\",\n",
    "    \"output_dir\": \"../../analysis_data/stan_fit\",\n",
    "}\n",
    "config[\"output_dir\"] = f\"../../analysis_data/stan_fits/stan_fit_{config[\"eval_sample_size\"]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(mu):\n",
    "    \"\"\"Calculate the log sum of exponentials\n",
    "    (pulling out the max to maintain) precision.\n",
    "    \"\"\"\n",
    "    mu_max = np.max(mu)\n",
    "\n",
    "    summation = np.sum(np.exp(mu - mu_max))\n",
    "    return mu_max + np.log(summation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_ppd(log_p):\n",
    "    M = len(log_p)\n",
    "    return -np.log(M) + log_sum_exp(log_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"../../analysis_data/all_data.csv\")\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic nan handling across all obs\n",
    "df_all[\"log_cpy\"] = np.log10(df_all[\"citations_per_year\"])\n",
    "df_all[\"log_cpy\"] = df_all[\"log_cpy\"].replace(-np.inf, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the field category more useful\n",
    "df_all[\"fields_of_study_0\"] = df_all[\"fields_of_study_0\"].astype(\"category\")\n",
    "df_all[\"stan_field_code\"] = df_all[\"fields_of_study_0\"].cat.codes + 1\n",
    "field_names = df_all[\"fields_of_study_0\"].cat.categories\n",
    "field_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_names = sorted(df_all[\"vectorizer\"].unique())\n",
    "vectorizer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the output dir and recreate\n",
    "if os.path.exists(config[\"output_dir\"]):\n",
    "    shutil.rmtree(config[\"output_dir\"])\n",
    "os.makedirs(config[\"output_dir\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Modeling\n",
    "\n",
    "Note to future self: cmdstanpy and cmdstan advertise conda as *the* way to install.\n",
    "Don't listen to them.\n",
    "I couldn't get it to compile when I used conda for anything cmdstanpy related.\n",
    "I downloaded the repo and compiled manually, and used pip for cmdstanpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to wide format so we can use the same train_test division\n",
    "# and so we can use multiple densities at once\n",
    "df_vectorizers = df_all.pivot_table(\n",
    "    index=\"identifier\", columns=\"vectorizer\", values=\"density\"\n",
    ")\n",
    "# For the nonden cols we can take the first because the other values are duplicates\n",
    "nonden_cols = [\n",
    "    \"log_cpy\",\n",
    "    config[\"cat_col\"],\n",
    "] + config[\"nonden_fit_cols\"]\n",
    "df_others = df_all.groupby(\"identifier\")[nonden_cols].first()\n",
    "df_eval = pd.concat([df_others, df_vectorizers], axis=\"columns\")\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns where log_cpy is na\n",
    "df_eval = df_eval.dropna(subset=\"log_cpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep to apply scaling (normalization by mean and sigma) to help with modelling\n",
    "scaled_cols = (\n",
    "    [\n",
    "        \"log_cpy\",\n",
    "    ]\n",
    "    + config[\"nonden_fit_cols\"]\n",
    "    + vectorizer_names\n",
    ")\n",
    "# Convert any non-float to float so scaling doesn't throw errors\n",
    "df_eval.loc[:, scaled_cols] = df_eval[scaled_cols].astype('float')\n",
    "df_eval_copy = df_eval.copy() # Make a copy before modifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global scaling\n",
    "scaler = StandardScaler()\n",
    "df_eval.loc[:, scaled_cols] = scaler.fit_transform(df_eval[scaled_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply per-field scaling\n",
    "scalers = {}\n",
    "new_cols = [f\"{col}_j\" for col in scaled_cols]\n",
    "for code_i, ids_i in df_eval_copy.groupby(\"stan_field_code\").groups.items():\n",
    "    df_i = df_eval_copy.loc[ids_i]\n",
    "\n",
    "    # Scale\n",
    "    scaler_i = StandardScaler()\n",
    "    df_eval.loc[ids_i, new_cols] = scaler_i.fit_transform(df_i[scaled_cols])\n",
    "    scalers[code_i] = scaler_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for referring to scaled versions of the columns\n",
    "scaled_nonden_fit_cols = [f\"{col}_j\" for col in config[\"nonden_fit_cols\"]]\n",
    "scaled_vectorizer_cols = [f\"{vectorizer_i}_j\" for vectorizer_i in vectorizer_names]\n",
    "scaled_fit_cols = scaled_nonden_fit_cols + scaled_vectorizer_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample\n",
    "if config[\"eval_sample_size\"] is not None:\n",
    "    df_eval = df_eval.sample(config[\"eval_sample_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "df_train, df_test = train_test_split(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later analysis\n",
    "df_train.to_csv(f\"{config[\"output_dir\"]}/train_data.csv\")\n",
    "df_test.to_csv(f\"{config[\"output_dir\"]}/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store results in\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model\n",
    "The base model is just a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"base\"\n",
    "model_name = \"base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for storing output\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df_train_i = df_train.dropna(subset=[\"log_cpy\"])\n",
    "df_test_i = df_train.dropna(subset=[\"log_cpy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "data = {\n",
    "    \"N\": len(df_train_i),\n",
    "    \"y\": df_train_i[\"log_cpy\"].values,\n",
    "    \"N_test\": len(df_test_i),\n",
    "    \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "if not config[\"output_dir\"] is None:\n",
    "    output_dir = os.path.join(config[\"output_dir\"], model_name)\n",
    "fit = model.sample(\n",
    "    data=data,\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse output\n",
    "draws = fit.draws_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store parameters\n",
    "output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "output[\"sigma\"] = draws[\"sigma\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log posterior predictive density\n",
    "output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[stan_model] = pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear on one density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"reg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold modeling output\n",
    "\n",
    "# Loop through vectorizers\n",
    "outputs_for_this_model = {}\n",
    "for i, vectorizer_i in enumerate(vectorizer_names):\n",
    "    output = {}\n",
    "\n",
    "    model_name = f\"{vectorizer_i}\"\n",
    "\n",
    "    # Drop na\n",
    "    df_train_i = df_train.dropna(subset=[vectorizer_i, \"log_cpy\"])\n",
    "    df_test_i = df_train.dropna(subset=[vectorizer_i, \"log_cpy\"])\n",
    "\n",
    "    # Format data\n",
    "    data = {\n",
    "        \"N\": len(df_train_i),\n",
    "        \"x\": df_train_i[vectorizer_i].values,\n",
    "        \"y\": df_train_i[\"log_cpy\"].values,\n",
    "        \"N_test\": len(df_test_i),\n",
    "        \"x_test\": df_test_i[vectorizer_i].values,\n",
    "        \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "    }\n",
    "\n",
    "    # Compile model\n",
    "    model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "    # Fit model\n",
    "    if not config[\"output_dir\"] is None:\n",
    "        output_dir = os.path.join(config[\"output_dir\"], model_name)\n",
    "    fit = model.sample(\n",
    "        data=data,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    # Parse output\n",
    "    draws = fit.draws_pd()\n",
    "\n",
    "    # Store parameters\n",
    "    output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "    output[f\"beta[{vectorizer_i}]\"] = draws[\"beta\"].median()\n",
    "    output[\"sigma\"] = draws[\"sigma\"].median()\n",
    "\n",
    "    # Calculate log posterior predictive density\n",
    "    output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])\n",
    "\n",
    "    outputs_for_this_model[model_name] = output\n",
    "    results_dict[model_name] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for showing here\n",
    "output = pd.DataFrame(outputs_for_this_model).T\n",
    "output[\"vectorizer\"] = vectorizer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medianplot(data, x, y, bins, ax, alpha_min=0.1, alpha_max=1.0):\n",
    "\n",
    "    x = data[x]\n",
    "    y = data[y]\n",
    "\n",
    "    centers = (bins[1:] + bins[:-1]) / 2\n",
    "\n",
    "    # TODO: Use or remove. Needs a LineCollection to work\n",
    "    # # Calculate histogram (used for line transparency)\n",
    "    # hist, _ = np.histogram(x, bins=bins, density=True)\n",
    "    # hist /= hist.max()\n",
    "    # alpha = alpha_min + hist * (alpha_max - alpha_min)\n",
    "\n",
    "    # Calculate running median\n",
    "    median, bin_edges, bin_number = binned_statistic(\n",
    "        x, y, statistic=np.nanmedian, bins=bins\n",
    "    )\n",
    "    ax.plot(\n",
    "        centers,\n",
    "        median,\n",
    "        color=\"k\",\n",
    "        # alpha=alpha,\n",
    "    )\n",
    "\n",
    "    # Calculate running percentiles\n",
    "    low, bin_edges, bin_number = binned_statistic(\n",
    "        x,\n",
    "        y,\n",
    "        statistic=lambda v: np.nanpercentile(v, 16),\n",
    "        bins=bins,\n",
    "    )\n",
    "    high, bin_edges, bin_number = binned_statistic(\n",
    "        x,\n",
    "        y,\n",
    "        statistic=lambda v: np.nanpercentile(v, 84),\n",
    "        bins=bins,\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        centers,\n",
    "        low,\n",
    "        high,\n",
    "        color=\"k\",\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "    ax.set_xlim(bins[0], bins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_edge = np.nanpercentile(\n",
    "    df_eval[vectorizer_names].values.flatten(), config[\"min_bin_percentile\"]\n",
    ")\n",
    "right_edge = -left_edge\n",
    "bins = np.linspace(left_edge, right_edge, config[\"bins\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in output.iterrows():\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "\n",
    "    df_plot = df_eval.dropna(subset=[row[\"vectorizer\"], \"log_cpy\"])\n",
    "\n",
    "    medianplot(\n",
    "        data=df_plot,\n",
    "        x=row[\"vectorizer\"],\n",
    "        y=\"log_cpy\",\n",
    "        bins=bins,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Plot the regression\n",
    "    xs = bins\n",
    "    ys = row[\"alpha\"] + row[f\"beta[{row[\"vectorizer\"]}]\"] * xs\n",
    "    ax.plot(\n",
    "        xs,\n",
    "        ys,\n",
    "        color=palette[0],\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        xs,\n",
    "        ys - row[\"sigma\"],\n",
    "        ys + row[\"sigma\"],\n",
    "        color=palette[0],\n",
    "        alpha=0.4,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(f\"density [{row[\"vectorizer\"]}]\")\n",
    "    ax.set_ylabel(\"log_cpy\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear on non-density vars\n",
    "The next model is a multivate linear regression with no density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"multireg\"\n",
    "model_name = \"nondens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_cols = config[\"nonden_fit_cols\"]\n",
    "cols = [\n",
    "    \"log_cpy\",\n",
    "] + fit_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for storing output\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df_train_i = df_train[cols].dropna()\n",
    "df_test_i = df_train[cols].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "data = {\n",
    "    \"N\": len(df_train_i),\n",
    "    \"K\": len(fit_cols),\n",
    "    \"x\": df_train_i[fit_cols].values,\n",
    "    \"y\": df_train_i[\"log_cpy\"].values,\n",
    "    \"N_test\": len(df_test_i),\n",
    "    \"x_test\": df_test_i[fit_cols].values,\n",
    "    \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "if not config[\"output_dir\"] is None:\n",
    "    output_dir = os.path.join(config[\"output_dir\"], model_name)\n",
    "fit = model.sample(\n",
    "    data=data,\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse output\n",
    "draws = fit.draws_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store parameters\n",
    "output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "output[\"sigma\"] = draws[\"sigma\"].median()\n",
    "beta_cols = [col for col in draws.columns if col[:4] == \"beta\"]\n",
    "betas = draws[beta_cols].median(axis=\"rows\")\n",
    "for i, fit_col in enumerate(fit_cols):\n",
    "    output[f\"beta[{fit_col}]\"] = betas.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log posterior predictive density\n",
    "output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[model_name] = pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear on one density + non-density vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"multireg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold modeling output\n",
    "\n",
    "# Loop through vectorizers\n",
    "outputs_for_this_model = {}\n",
    "for i, vectorizer_i in enumerate(vectorizer_names):\n",
    "    output = {}\n",
    "\n",
    "    model_name = f\"{vectorizer_i} + nondens\"\n",
    "\n",
    "    # Drop na\n",
    "    fit_cols = [\n",
    "        vectorizer_i,\n",
    "    ] + config[\"nonden_fit_cols\"]\n",
    "    cols = [\n",
    "        \"log_cpy\",\n",
    "    ] + fit_cols\n",
    "    df_train_i = df_train.dropna(subset=cols)\n",
    "    df_test_i = df_train.dropna(subset=cols)\n",
    "\n",
    "    # Format data\n",
    "    data = {\n",
    "        \"N\": len(df_train_i),\n",
    "        \"K\": len(fit_cols),\n",
    "        \"x\": df_train_i[fit_cols].values,\n",
    "        \"y\": df_train_i[\"log_cpy\"].values,\n",
    "        \"N_test\": len(df_test_i),\n",
    "        \"x_test\": df_test_i[fit_cols].values,\n",
    "        \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "    }\n",
    "\n",
    "    # Compile model\n",
    "    model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "    # Fit model\n",
    "    if not config[\"output_dir\"] is None:\n",
    "        output_dir = os.path.join(config[\"output_dir\"], model_name)\n",
    "    fit = model.sample(\n",
    "        data=data,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    # Parse output\n",
    "    draws = fit.draws_pd()\n",
    "\n",
    "    # Store parameters\n",
    "    output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "    output[\"sigma\"] = draws[\"sigma\"].median()\n",
    "    beta_cols = [col for col in draws.columns if col[:4] == \"beta\"]\n",
    "    betas = draws[beta_cols].median(axis=\"rows\")\n",
    "    for i, fit_col in enumerate(fit_cols):\n",
    "        output[f\"beta[{fit_col}]\"] = betas.iloc[i]\n",
    "\n",
    "    # Calculate log posterior predictive density\n",
    "    output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])\n",
    "\n",
    "    outputs_for_this_model[model_name] = output\n",
    "    results_dict[model_name] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear on all densities + non-density vars\n",
    "The next model is a multivate linear regression using every variable we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"multireg\"\n",
    "model_name = \"dens + nondens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_cols = config[\"nonden_fit_cols\"] + vectorizer_names\n",
    "cols = [\n",
    "    \"log_cpy\",\n",
    "] + fit_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict for storing output\n",
    "output = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop na\n",
    "df_train_i = df_train[cols].dropna()\n",
    "df_test_i = df_train[cols].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data\n",
    "data = {\n",
    "    \"N\": len(df_train_i),\n",
    "    \"K\": len(fit_cols),\n",
    "    \"x\": df_train_i[fit_cols].values,\n",
    "    \"y\": df_train_i[\"log_cpy\"].values,\n",
    "    \"N_test\": len(df_test_i),\n",
    "    \"x_test\": df_test_i[fit_cols].values,\n",
    "    \"y_test\": df_test_i[\"log_cpy\"].values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "if not config[\"output_dir\"] is None:\n",
    "    output_dir = os.path.join(config[\"output_dir\"], model_name)\n",
    "fit = model.sample(\n",
    "    data=data,\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse output\n",
    "draws = fit.draws_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store parameters\n",
    "output[\"alpha\"] = draws[\"alpha\"].median()\n",
    "output[\"sigma\"] = draws[\"sigma\"].median()\n",
    "beta_cols = [col for col in draws.columns if col[:4] == \"beta\"]\n",
    "betas = draws[beta_cols].median(axis=\"rows\")\n",
    "for i, fit_col in enumerate(fit_cols):\n",
    "    output[f\"beta[{fit_col}]\"] = betas.iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log posterior predictive density\n",
    "output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[model_name] = pd.Series(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical on non-density vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"hreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"log_cpy_j\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    \"y_col\": y_col,\n",
    "}\n",
    "\n",
    "model_name = f\"h-nondens\"\n",
    "\n",
    "# Drop na\n",
    "df_train_i = df_train.dropna()\n",
    "df_test_i = df_train.dropna()\n",
    "\n",
    "# Format data\n",
    "data = {\n",
    "    \"D\": len(scaled_nonden_fit_cols),\n",
    "    \"N\": len(df_train_i),\n",
    "    \"L\": df_train_i[config[\"cat_col\"]].max(),\n",
    "    \"y\": df_train_i[y_col].values,\n",
    "    \"ll\": df_train_i[config[\"cat_col\"]].values,\n",
    "    \"x\": df_train_i[scaled_nonden_fit_cols].values,\n",
    "    \"N_test\": len(df_test_i),\n",
    "    \"x_test\": df_test_i[scaled_nonden_fit_cols].values,\n",
    "    \"y_test\": df_test_i[y_col].values,\n",
    "}\n",
    "\n",
    "# Compile model\n",
    "model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "# Fit model\n",
    "if not config[\"output_dir\"] is None:\n",
    "    output_dir = os.path.join(config[\"output_dir\"], model_name)\n",
    "fit = model.sample(\n",
    "    data=data,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# Parse output\n",
    "stan_vars = fit.stan_variables()\n",
    "\n",
    "# Store parameters\n",
    "for param_key in [\"alpha\", \"sigma\"]:\n",
    "    output[param_key] = np.median(stan_vars[param_key])\n",
    "\n",
    "# Store per-field and per var betas\n",
    "betas = np.nanmedian(stan_vars[\"beta\"], axis=0)\n",
    "for k, fit_col_k in enumerate(scaled_nonden_fit_cols):\n",
    "    for j, field_name_j in enumerate(field_names):\n",
    "        output[f\"beta[{fit_col_k},{field_names[j]}]\"] = betas[j][k]\n",
    "\n",
    "# Calculate log posterior predictive density\n",
    "output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])\n",
    "\n",
    "outputs_for_this_model[model_name] = output\n",
    "results_dict[model_name] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical linear on one density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"hreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to per-field scalings\n",
    "y_col = \"log_cpy_j\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold modeling output\n",
    "\n",
    "# Loop through vectorizers\n",
    "outputs_for_this_model = {}\n",
    "for i, vectorizer_i in enumerate(scaled_vectorizer_cols):\n",
    "    output = {\n",
    "        \"y_col\": y_col,\n",
    "    }\n",
    "\n",
    "    model_name = f\"h-{vectorizer_i}\"\n",
    "\n",
    "    # Drop na\n",
    "    df_train_i = df_train.dropna(subset=[vectorizer_i, y_col])\n",
    "    df_test_i = df_train.dropna(subset=[vectorizer_i, y_col])\n",
    "\n",
    "    # Format data\n",
    "    data = {\n",
    "        \"D\": 1,\n",
    "        \"N\": len(df_train_i),\n",
    "        \"L\": df_train_i[config[\"cat_col\"]].max(),\n",
    "        \"y\": df_train_i[y_col].values,\n",
    "        \"ll\": df_train_i[config[\"cat_col\"]].values,\n",
    "        \"x\": df_train_i[[vectorizer_i]].values,\n",
    "        \"N_test\": len(df_test_i),\n",
    "        \"x_test\": df_test_i[[vectorizer_i]].values,\n",
    "        \"y_test\": df_test_i[y_col].values,\n",
    "    }\n",
    "\n",
    "    # Compile model\n",
    "    model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "    # Fit model\n",
    "    if not config[\"output_dir\"] is None:\n",
    "        output_dir = os.path.join(config[\"output_dir\"], model_name)\n",
    "    fit = model.sample(\n",
    "        data=data,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    # Parse output\n",
    "    stan_vars = fit.stan_variables()\n",
    "\n",
    "    # Store parameters\n",
    "    output[\"alpha\"] = np.median(stan_vars[\"alpha\"])\n",
    "    output[\"sigma\"] = np.median(stan_vars[\"sigma\"])\n",
    "\n",
    "    # Store per-field betas\n",
    "    betas = np.nanmedian(stan_vars[\"beta\"], axis=0)\n",
    "    for j, field_name_j in enumerate(field_names):\n",
    "        output[f\"beta[{vectorizer_i},{field_name_j}]\"] = betas[j][0]\n",
    "\n",
    "    # Calculate log posterior predictive density\n",
    "    output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])\n",
    "\n",
    "    outputs_for_this_model[model_name] = output\n",
    "    results_dict[model_name] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for showing here\n",
    "output = pd.DataFrame(outputs_for_this_model).T\n",
    "output[\"vectorizer\"] = scaled_vectorizer_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.4 * len(field_names), 4.8 * len(scaled_vectorizer_cols)))\n",
    "mosaic = [[f\"{vectorizer_i}_{field_j}\" for field_j in field_names] for vectorizer_i in scaled_vectorizer_cols]\n",
    "ax_dict = fig.subplot_mosaic(mosaic=mosaic)\n",
    "for i, (model_i, row) in enumerate(output.iterrows()):\n",
    "    vectorizer_i = row[\"vectorizer\"]\n",
    "    for j, field_j in enumerate(field_names):\n",
    "\n",
    "        ax_key = f\"{vectorizer_i}_{field_j}\" \n",
    "        ax = ax_dict[ax_key]\n",
    "\n",
    "        df_plot = df_eval.query(f\"stan_field_code == {j+1}\")\n",
    "\n",
    "        medianplot(\n",
    "            data=df_plot,\n",
    "            x=row[\"vectorizer\"],\n",
    "            y=y_col,\n",
    "            bins=bins,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        # Plot the regression\n",
    "        xs = bins\n",
    "        ys = row[\"alpha\"] + row[f\"beta[{row[\"vectorizer\"]},{field_j}]\"] * xs\n",
    "        ax.plot(\n",
    "            xs,\n",
    "            ys,\n",
    "            color=palette[0],\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            xs,\n",
    "            ys - row[\"sigma\"],\n",
    "            ys + row[\"sigma\"],\n",
    "            color=palette[0],\n",
    "            alpha=0.4,\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(f\"density [{row[\"vectorizer\"]},{field_j}]\")\n",
    "        ax.set_ylabel(y_col)\n",
    "\n",
    "        ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical linear on one density + nondenvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"hreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to per-field scalings\n",
    "y_col = \"log_cpy_j\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through vectorizers\n",
    "outputs_for_this_model = {}\n",
    "for i, vectorizer_i in enumerate(scaled_vectorizer_cols):\n",
    "    output = {\n",
    "        \"y_col\": y_col,\n",
    "    }\n",
    "\n",
    "    model_name = f\"h-{vectorizer_i} + h-nondens\"\n",
    "\n",
    "    # Drop na\n",
    "    df_train_i = df_train.dropna(subset=[vectorizer_i, y_col])\n",
    "    df_test_i = df_train.dropna(subset=[vectorizer_i, y_col])\n",
    "\n",
    "    fit_cols_i = scaled_nonden_fit_cols + [vectorizer_i, ]\n",
    "\n",
    "    # Format data\n",
    "    data = {\n",
    "        \"D\": len(fit_cols_i),\n",
    "        \"N\": len(df_train_i),\n",
    "        \"L\": df_train_i[config[\"cat_col\"]].max(),\n",
    "        \"y\": df_train_i[y_col].values,\n",
    "        \"ll\": df_train_i[config[\"cat_col\"]].values,\n",
    "        \"x\": df_train_i[fit_cols_i].values,\n",
    "        \"N_test\": len(df_test_i),\n",
    "        \"x_test\": df_test_i[fit_cols_i].values,\n",
    "        \"y_test\": df_test_i[y_col].values,\n",
    "    }\n",
    "\n",
    "    # Compile model\n",
    "    model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "    # Fit model\n",
    "    if not config[\"output_dir\"] is None:\n",
    "        output_dir = os.path.join(config[\"output_dir\"], model_name)\n",
    "    fit = model.sample(\n",
    "        data=data,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "\n",
    "    # Parse output\n",
    "    stan_vars = fit.stan_variables()\n",
    "\n",
    "    # Store parameters\n",
    "    output[\"alpha\"] = np.median(stan_vars[\"alpha\"])\n",
    "    output[\"sigma\"] = np.median(stan_vars[\"sigma\"])\n",
    "\n",
    "    # Store per-field and per var betas\n",
    "    betas = np.nanmedian(stan_vars[\"beta\"], axis=0)\n",
    "    for k, fit_col_k in enumerate(fit_cols_i):\n",
    "        for j, field_name_j in enumerate(field_names):\n",
    "            output[f\"beta[{fit_col_k},{field_names[j]}]\"] = betas[j][k]\n",
    "\n",
    "    # Calculate log posterior predictive density\n",
    "    output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])\n",
    "\n",
    "    outputs_for_this_model[model_name] = output\n",
    "    results_dict[model_name] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for showing here\n",
    "output = pd.DataFrame(outputs_for_this_model).T\n",
    "output[\"vectorizer\"] = scaled_vectorizer_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.4 * len(field_names), 4.8 * len(scaled_vectorizer_cols)))\n",
    "mosaic = [[f\"{vectorizer_i}_{field_j}\" for field_j in field_names] for vectorizer_i in scaled_vectorizer_cols]\n",
    "ax_dict = fig.subplot_mosaic(mosaic=mosaic)\n",
    "for i, (model_i, row) in enumerate(output.iterrows()):\n",
    "    vectorizer_i = row[\"vectorizer\"]\n",
    "    for j, field_j in enumerate(field_names):\n",
    "\n",
    "        ax_key = f\"{vectorizer_i}_{field_j}\" \n",
    "        ax = ax_dict[ax_key]\n",
    "\n",
    "        df_plot = df_eval.query(f\"stan_field_code == {j+1}\")\n",
    "\n",
    "        medianplot(\n",
    "            data=df_plot,\n",
    "            x=row[\"vectorizer\"],\n",
    "            y=y_col,\n",
    "            bins=bins,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        # Plot the regression\n",
    "        xs = bins\n",
    "        ys = row[\"alpha\"] + row[f\"beta[{row[\"vectorizer\"]},{field_j}]\"] * xs\n",
    "        ax.plot(\n",
    "            xs,\n",
    "            ys,\n",
    "            color=palette[0],\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            xs,\n",
    "            ys - row[\"sigma\"],\n",
    "            ys + row[\"sigma\"],\n",
    "            color=palette[0],\n",
    "            alpha=0.4,\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(f\"density [{row[\"vectorizer\"]},{field_j}]\")\n",
    "        ax.set_ylabel(y_col)\n",
    "\n",
    "        ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical on two densities + non-density vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"hreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"log_cpy_j\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_fit_cols = [_ for _ in scaled_fit_cols if _ in [\"BOW_j\", \"SciBERT_j\",] + scaled_nonden_fit_cols]\n",
    "used_fit_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    \"y_col\": y_col,\n",
    "}\n",
    "\n",
    "model_name = f\"h-BOW-SciBERT + h-nondens\"\n",
    "\n",
    "# Drop na\n",
    "df_train_i = df_train.dropna()\n",
    "df_test_i = df_train.dropna()\n",
    "\n",
    "# Format data\n",
    "data = {\n",
    "    \"D\": len(used_fit_cols),\n",
    "    \"N\": len(df_train_i),\n",
    "    \"L\": df_train_i[config[\"cat_col\"]].max(),\n",
    "    \"y\": df_train_i[y_col].values,\n",
    "    \"ll\": df_train_i[config[\"cat_col\"]].values,\n",
    "    \"x\": df_train_i[used_fit_cols].values,\n",
    "    \"N_test\": len(df_test_i),\n",
    "    \"x_test\": df_test_i[used_fit_cols].values,\n",
    "    \"y_test\": df_test_i[y_col].values,\n",
    "}\n",
    "\n",
    "# Compile model\n",
    "model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "# Fit model\n",
    "if not config[\"output_dir\"] is None:\n",
    "    output_dir = os.path.join(config[\"output_dir\"], model_name)\n",
    "fit = model.sample(\n",
    "    data=data,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# Parse output\n",
    "stan_vars = fit.stan_variables()\n",
    "\n",
    "# Store parameters\n",
    "for param_key in [\"alpha\", \"sigma\"]:\n",
    "    output[param_key] = np.median(stan_vars[param_key])\n",
    "\n",
    "# Store per-field and per var betas\n",
    "betas = np.nanmedian(stan_vars[\"beta\"], axis=0)\n",
    "for k, fit_col_k in enumerate(used_fit_cols):\n",
    "    for j, field_name_j in enumerate(field_names):\n",
    "        output[f\"beta[{fit_col_k},{field_names[j]}]\"] = betas[j][k]\n",
    "\n",
    "# Calculate log posterior predictive density\n",
    "output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])\n",
    "\n",
    "outputs_for_this_model[model_name] = output\n",
    "results_dict[model_name] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.4 * len(field_names), 4.8 * len(vectorizer_names)))\n",
    "mosaic = [[f\"{fit_col_i}_{field_j}\" for field_j in field_names] for fit_col_i in scaled_fit_cols]\n",
    "ax_dict = fig.subplot_mosaic(mosaic=mosaic)\n",
    "for i, fit_col_i in enumerate(used_fit_cols):\n",
    "    for j, field_j in enumerate(field_names):\n",
    "\n",
    "        ax_key = f\"{fit_col_i}_{field_j}\" \n",
    "        ax = ax_dict[ax_key]\n",
    "\n",
    "        df_plot = df_eval.query(f\"stan_field_code == {j+1}\")\n",
    "\n",
    "        medianplot(\n",
    "            data=df_plot,\n",
    "            x=fit_col_i,\n",
    "            y=y_col,\n",
    "            bins=bins,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        # Plot the regression\n",
    "        xs = bins\n",
    "        ys = output[\"alpha\"] + output[f\"beta[{fit_col_i},{field_j}]\"] * xs\n",
    "        ax.plot(\n",
    "            xs,\n",
    "            ys,\n",
    "            color=palette[0],\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            xs,\n",
    "            ys - output[\"sigma\"],\n",
    "            ys + output[\"sigma\"],\n",
    "            color=palette[0],\n",
    "            alpha=0.4,\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(f\"density [{fit_col_i},{field_j}]\")\n",
    "        ax.set_ylabel(y_col)\n",
    "\n",
    "        ax.set_ylim(-1.5, 1.5)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical on all densities + non-density vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model = \"hreg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = \"log_cpy_j\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    \"y_col\": y_col,\n",
    "}\n",
    "\n",
    "model_name = f\"h-dens + h-nondens\"\n",
    "\n",
    "# Drop na\n",
    "df_train_i = df_train.dropna()\n",
    "df_test_i = df_train.dropna()\n",
    "\n",
    "# Format data\n",
    "data = {\n",
    "    \"D\": len(scaled_fit_cols),\n",
    "    \"N\": len(df_train_i),\n",
    "    \"L\": df_train_i[config[\"cat_col\"]].max(),\n",
    "    \"y\": df_train_i[y_col].values,\n",
    "    \"ll\": df_train_i[config[\"cat_col\"]].values,\n",
    "    \"x\": df_train_i[scaled_fit_cols].values,\n",
    "    \"N_test\": len(df_test_i),\n",
    "    \"x_test\": df_test_i[scaled_fit_cols].values,\n",
    "    \"y_test\": df_test_i[y_col].values,\n",
    "}\n",
    "\n",
    "# Compile model\n",
    "model = cmdstanpy.CmdStanModel(stan_file=f\"../stan_models/{stan_model}.stan\")\n",
    "\n",
    "# Fit model\n",
    "if not config[\"output_dir\"] is None:\n",
    "    output_dir = os.path.join(config[\"output_dir\"], model_name)\n",
    "fit = model.sample(\n",
    "    data=data,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# Parse output\n",
    "stan_vars = fit.stan_variables()\n",
    "\n",
    "# Store parameters\n",
    "for param_key in [\"alpha\", \"sigma\"]:\n",
    "    output[param_key] = np.median(stan_vars[param_key])\n",
    "\n",
    "# Store per-field and per var betas\n",
    "betas = np.nanmedian(stan_vars[\"beta\"], axis=0)\n",
    "for k, fit_col_k in enumerate(scaled_fit_cols):\n",
    "    for j, field_name_j in enumerate(field_names):\n",
    "        output[f\"beta[{fit_col_k},{field_names[j]}]\"] = betas[j][k]\n",
    "\n",
    "# Calculate log posterior predictive density\n",
    "output[\"log_ppd\"] = log_ppd(draws[\"log_p\"])\n",
    "\n",
    "outputs_for_this_model[model_name] = output\n",
    "results_dict[model_name] = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.4 * len(field_names), 4.8 * len(vectorizer_names)))\n",
    "mosaic = [[f\"{fit_col_i}_{field_j}\" for field_j in field_names] for fit_col_i in scaled_fit_cols]\n",
    "ax_dict = fig.subplot_mosaic(mosaic=mosaic)\n",
    "for i, fit_col_i in enumerate(scaled_fit_cols):\n",
    "    for j, field_j in enumerate(field_names):\n",
    "\n",
    "        ax_key = f\"{fit_col_i}_{field_j}\" \n",
    "        ax = ax_dict[ax_key]\n",
    "\n",
    "        df_plot = df_eval.query(f\"stan_field_code == {j+1}\")\n",
    "\n",
    "        medianplot(\n",
    "            data=df_plot,\n",
    "            x=fit_col_i,\n",
    "            y=y_col,\n",
    "            bins=bins,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "        # Plot the regression\n",
    "        xs = bins\n",
    "        ys = output[\"alpha\"] + output[f\"beta[{fit_col_i},{field_j}]\"] * xs\n",
    "        ax.plot(\n",
    "            xs,\n",
    "            ys,\n",
    "            color=palette[0],\n",
    "        )\n",
    "        ax.fill_between(\n",
    "            xs,\n",
    "            ys - output[\"sigma\"],\n",
    "            ys + output[\"sigma\"],\n",
    "            color=palette[0],\n",
    "            alpha=0.4,\n",
    "        )\n",
    "\n",
    "        ax.set_xlabel(f\"density [{fit_col_i},{field_j}]\")\n",
    "        ax.set_ylabel(y_col)\n",
    "\n",
    "        ax.set_ylim(-1.5, 1.5)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where we didn't specify a y column we default to log_cpy\n",
    "results[\"y_col\"] = results[\"y_col\"].fillna(\"log_cpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results.to_csv(f\"{config[\"output_dir\"]}/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test x values\n",
    "x_test = df_test[fit_cols].fillna(0.0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the contribution to y_pred from the intercept\n",
    "alphas = results[\"alpha\"].values.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients for regular regression\n",
    "# and the contribution to y_pred from the regular regression\n",
    "beta_cols = [f\"beta[{fit_col}]\" for fit_col in fit_cols]\n",
    "betas = results[beta_cols].fillna(0.0).values\n",
    "y_pred_beta = np.matmul(x_test, betas.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and format y_pred (without the hierarchical reg contribution)\n",
    "y_pred = alphas + y_pred_beta\n",
    "y_pred = pd.DataFrame(y_pred, index=df_test.index, columns=results.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the contribution to y_pred from hierarchical regression\n",
    "n_tot = 0\n",
    "for i, field_i in enumerate(field_names):\n",
    "    # Get the x values\n",
    "    df_test_i = df_test.query(f\"stan_field_code == {i + 1}\")\n",
    "    x_test_i = df_test_i[scaled_fit_cols].fillna(0.0).values\n",
    "\n",
    "    # Get the slopes used for this particular field\n",
    "    beta_cols_i = [\n",
    "        f\"beta[{fit_col_j},{field_i}]\" for fit_col_j in scaled_fit_cols\n",
    "    ]\n",
    "    betas_j = results[beta_cols_i].fillna(0.0).values\n",
    "\n",
    "    # Get y_pred and store it\n",
    "    y_pred_beta_i = np.matmul(x_test_i, betas_j.T)\n",
    "    y_pred.loc[df_test_i.index] += y_pred_beta_i\n",
    "\n",
    "    # Keep track of n_tot as a double check that every observation has a contribution\n",
    "    # from their primary field\n",
    "    n_tot += len(y_pred_beta_i)\n",
    "\n",
    "assert n_tot == len(df_test), \"y_pred count doesn't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate original non-scaled y_pred\n",
    "y_pred_orig = y_pred.copy()\n",
    "y_pred_orig = y_pred_orig.set_index([df_test[\"stan_field_code\"], y_pred_orig.index]).sort_index()\n",
    "for i, field_i in enumerate(field_names):\n",
    "    y_pred_i = y_pred_orig.loc[i+1].copy()\n",
    "    for j, model_j in enumerate(results.index):\n",
    "        if results.loc[model_j,\"y_col\"] == \"log_cpy\":\n",
    "            scaler_used = scaler\n",
    "        if results.loc[model_j,\"y_col\"] == \"log_cpy_j\":\n",
    "            scaler_used = scalers[i+1]\n",
    "        y_pred_orig_ij = scaler_used.scale_[0] * y_pred_i[model_j] + scaler_used.mean_[0]\n",
    "        y_pred_i[model_j] = y_pred_orig_ij\n",
    "    y_pred_orig.loc[i+1] = y_pred_i.values\n",
    "\n",
    "# Put the entries back where we found them\n",
    "y_pred_orig = y_pred_orig.reset_index().set_index(\"identifier\").loc[y_pred.index]\n",
    "y_pred_orig = y_pred_orig.drop(\"stan_field_code\", axis=\"columns\")\n",
    "y_pred_orig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Posterior Predictive Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6.4 * len(results) / 10, 4.8))\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.arange(len(results))\n",
    "ax.scatter(\n",
    "    x,\n",
    "    results[\"log_ppd\"] - results.loc[\"base\", \"log_ppd\"],\n",
    ")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results.index, rotation=45, ha=\"right\")\n",
    "\n",
    "ax.set_ylabel(r\"PPD $\\log$ likelihood ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rmse\n",
    "rmse = []\n",
    "for i, y_col_i in enumerate(results[\"y_col\"]):\n",
    "    model_i = results.index[i]\n",
    "    rmse_i = root_mean_squared_error(df_test[y_col_i], y_pred[model_i])\n",
    "    rmse.append(rmse_i)\n",
    "results[\"rmse\"] = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.arange(len(results))\n",
    "ax.scatter(\n",
    "    x,\n",
    "    results[\"rmse\"],\n",
    ")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results.index, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_betas = results[beta_cols].max(axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.arange(len(results))\n",
    "ax.scatter(\n",
    "    x,\n",
    "    max_betas,\n",
    ")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(results.index, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(r\"max($\\beta_i$)\")\n",
    "\n",
    "ax.set_ylim(0, ax.get_ylim()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max slopes correlation with rmse\n",
    "ax = sns.scatterplot(\n",
    "    results,\n",
    "    x=max_betas,\n",
    "    y=1 - results[\"rmse\"],\n",
    ")\n",
    "ax.set_xlim(0, ax.get_xlim()[1])\n",
    "ax.set_ylim(0, ax.get_ylim()[1])\n",
    "\n",
    "ax.set_xlabel(r\"$\\max(\\beta_i)$\")\n",
    "ax.set_ylabel(r\"1 - rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE vs distance from center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect rmse as a function of distance from the center\n",
    "for model_i in results.index:\n",
    "\n",
    "    row = results.loc[model_i]\n",
    "\n",
    "    # Only do models that have a density dependence\n",
    "    try:\n",
    "        vectorizer_i = model_i.split(\"_\")[-1]\n",
    "        assert vectorizer_i in vectorizer_names\n",
    "    except AssertionError:\n",
    "        continue\n",
    "\n",
    "    # Bin and labels setup\n",
    "    bins = np.linspace(-3, 3, config[\"bins\"])\n",
    "    centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    den_key = f\"rho_{vectorizer_i}\"\n",
    "    cut_key = f\"{den_key}_bin\"\n",
    "    model_key = f\"y_pred_{model_i}\"\n",
    "\n",
    "    # Format data into a new df and group it\n",
    "    df_test_to_group = pd.DataFrame()\n",
    "    df_test_to_group[den_key] = df_test[vectorizer_i]\n",
    "    df_test_to_group[model_key] = y_pred[model_i]\n",
    "    df_test_to_group[cut_key] = pd.cut(df_test_to_group[den_key], bins)\n",
    "    df_test_to_group[\"y_test\"] = df_test[row[\"y_col\"]]\n",
    "    df_grouped = df_test_to_group.groupby(cut_key)\n",
    "\n",
    "    # Get mean predicted y per bin\n",
    "    def get_mean_y_pred(df):\n",
    "        if len(df) == 0:\n",
    "            return np.nan\n",
    "        return df[model_key].mean()\n",
    "\n",
    "    mean_y_pred = df_grouped.apply(get_mean_y_pred)\n",
    "\n",
    "    # Get rmse\n",
    "    def get_rmse_of_df(df):\n",
    "        if len(df) == 0:\n",
    "            return np.nan\n",
    "        return root_mean_squared_error(df[\"y_test\"], df[model_key])\n",
    "\n",
    "    rmse_per_bin = df_grouped.apply(get_rmse_of_df)\n",
    "\n",
    "    # Also get rmse if y_pred was just the mean\n",
    "    def get_rmse_from_mean(df):\n",
    "        if len(df) == 0:\n",
    "            return np.nan\n",
    "        y_pred = np.zeros(len(df))  # Since we normalized to 0\n",
    "        return root_mean_squared_error(df[\"y_test\"], y_pred)\n",
    "\n",
    "    rmse_from_mean = df_grouped.apply(get_rmse_from_mean)\n",
    "\n",
    "    # And rmse expected if it was a straight improvement on the mean that scales with\n",
    "    # slope\n",
    "    min_rmse = rmse_from_mean - np.abs(centers * row[f\"beta[{vectorizer_i}]\"])\n",
    "\n",
    "    # Plot\n",
    "    fig, ax_dict = plt.subplot_mosaic(\n",
    "        [\n",
    "            [\n",
    "                \"hist\",\n",
    "            ],\n",
    "            [\n",
    "                \"reg\",\n",
    "            ],\n",
    "            [\n",
    "                \"reg\",\n",
    "            ],\n",
    "            [\n",
    "                \"rmse\",\n",
    "            ],\n",
    "            [\n",
    "                \"rmse\",\n",
    "            ],\n",
    "        ],\n",
    "        figsize=(8, 12),\n",
    "    )\n",
    "\n",
    "    # Hist plot\n",
    "    ax = ax_dict[\"hist\"]\n",
    "    sns.histplot(\n",
    "        x=df_test_to_group[den_key],\n",
    "        ax=ax,\n",
    "        bins=bins,\n",
    "    )\n",
    "    ax.set_xlim(bins[0], bins[-1])\n",
    "    ax.set_title(model_i)\n",
    "    ax.set_xlabel(None)\n",
    "\n",
    "    # Regression plot\n",
    "    ax = ax_dict[\"reg\"]\n",
    "\n",
    "    # Plot the median\n",
    "    medianplot(\n",
    "        data=df_test_to_group,\n",
    "        x=den_key,\n",
    "        y=\"y_test\",\n",
    "        bins=bins,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Plot the regression\n",
    "    xs = bins\n",
    "    ys = row[\"alpha\"] + row[f\"beta[{vectorizer_i}]\"] * xs\n",
    "    ax.plot(\n",
    "        xs,\n",
    "        ys,\n",
    "        color=palette[0],\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        xs,\n",
    "        ys - row[\"sigma\"],\n",
    "        ys + row[\"sigma\"],\n",
    "        color=palette[0],\n",
    "        alpha=0.25,\n",
    "    )\n",
    "\n",
    "    # Plot what y_pred actually is\n",
    "    ax = sns.scatterplot(\n",
    "        x=centers,\n",
    "        y=mean_y_pred,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(None)\n",
    "    ax.set_ylabel(\"log_cpy\")\n",
    "\n",
    "    # RMSE plot\n",
    "    ax = ax_dict[\"rmse\"]\n",
    "    ax = sns.scatterplot(\n",
    "        x=centers,\n",
    "        y=rmse_per_bin,\n",
    "        ax=ax,\n",
    "        label=\"rmse\",\n",
    "    )\n",
    "    ax = sns.scatterplot(x=centers, y=rmse_from_mean, ax=ax, label=\"rmse(y_pred=0)\")\n",
    "    ax = sns.scatterplot(x=centers, y=min_rmse, ax=ax, label=\"min rmse\")\n",
    "    ax.legend()\n",
    "    # Center line\n",
    "    ax.axvline(\n",
    "        0,\n",
    "        c=\"0.2\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    # Global rmse line\n",
    "    ax.axhline(\n",
    "        results.loc[model_i, \"rmse\"],\n",
    "        c=\"0.2\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "\n",
    "    ax.set_ylabel(\"rmse\")\n",
    "    ax.set_xlabel(den_key)\n",
    "\n",
    "    ax.set_xlim(bins[0], bins[-1])\n",
    "    ax.set_ylim(0, ax.get_ylim()[1])\n",
    "    fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that RMSE is capturing two behaviors: distance from the mean and also width of the distribution.\n",
    "The width of the distribution largely dominates over the distance from the mean.\n",
    "So instead we may want a metric that is an evaluation of the distance median.\n",
    "This might relate to Nathaniel's choice to bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    r\"$R^2$\": sk_metrics.r2_score,\n",
    "    \"1-MAE\": lambda y_true, y_pred: 1 - sk_metrics.mean_absolute_error(y_true, y_pred),\n",
    "    \"1-RMSE\": lambda y_true, y_pred: 1\n",
    "    - sk_metrics.root_mean_squared_error(y_true, y_pred),\n",
    "    \"1-MAPE\": lambda y_true, y_pred: 1\n",
    "    - sk_metrics.mean_absolute_percentage_error(y_true, y_pred),\n",
    "    \"1-MedAE\": lambda y_true, y_pred: 1\n",
    "    - sk_metrics.median_absolute_error(y_true, y_pred),\n",
    "    \"1-MPinL\": lambda y_true, y_pred: 1 - sk_metrics.mean_pinball_loss(y_true, y_pred),\n",
    "    r\"$D^2_{pin}$\": sk_metrics.d2_pinball_score,\n",
    "    r\"$D^2_{abs}$\": sk_metrics.d2_absolute_error_score,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_label, metric_fn in metrics.items():\n",
    "\n",
    "    # Calculate the metric\n",
    "    metric = []\n",
    "    for i, model_i in enumerate(results.index):\n",
    "        metric_i = metric_fn(df_test[results.loc[model_i, \"y_col\"]], y_pred[model_i])\n",
    "        metric.append(metric_i)\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(6.8 * len(results) / 10, 4.8))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    x = np.arange(len(results))\n",
    "    ax.scatter(\n",
    "        x,\n",
    "        metric,\n",
    "    )\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(results.index, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(metric_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many metrics rescaled\n",
    "Back in original units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_eval_copy.loc[df_test.index, \"log_cpy\"]\n",
    "\n",
    "for metric_label, metric_fn in metrics.items():\n",
    "    # Calculate the metric\n",
    "    metric = []\n",
    "    for i, model_i in enumerate(results.index):\n",
    "        metric_i = metric_fn(y_test, y_pred_orig[model_i])\n",
    "        metric.append(metric_i)\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(6.8 * len(results) / 10, 4.8))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    x = np.arange(len(results))\n",
    "    ax.scatter(\n",
    "        x,\n",
    "        metric,\n",
    "    )\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(results.index, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(metric_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citesim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
