{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Modeling\n",
    "What models fit the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"bins\": 16,\n",
    "    \"min_bin_percentile\": 10,\n",
    "    \"eval_sample_size\": 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"../../analysis_data/all_data.csv\")\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic nan handling across all obs\n",
    "df_all[\"log_cpy\"] = np.log10(df_all[\"citations_per_year\"])\n",
    "df_all['log_cpy'] = df_all['log_cpy'].replace(-np.inf, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = sorted(df_all[\"fields_of_study_0\"].unique())\n",
    "field_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_names = sorted(df_all[\"vectorizer\"].unique())\n",
    "vectorizer_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief aside on logscale plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "x = np.logspace(-3, 3, 100)\n",
    "log10 = np.log10(x)\n",
    "log10p = np.log10(1 + x)\n",
    "log10p2 = np.log10(1 + x) - 1\n",
    "\n",
    "ax.plot(\n",
    "    x,\n",
    "    log10,\n",
    "    label=\"log10\",\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    log10p,\n",
    "    label=\"log10p\",\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    log10p2,\n",
    "    label=\"log10p - 1\",\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview Normalized 2D Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bins(x, n_bins=config[\"bins\"]):\n",
    "    # Get density bins\n",
    "    bins = np.linspace(\n",
    "        np.nanpercentile(x, config[\"min_bin_percentile\"]),\n",
    "        np.nanpercentile(x, 100 - config[\"min_bin_percentile\"]),\n",
    "        n_bins + 1,\n",
    "    )\n",
    "    return bins\n",
    "\n",
    "\n",
    "density_bins = df_all.groupby(\"vectorizer\")[\"density\"].apply(get_bins).to_dict()\n",
    "cpy_bins = get_bins(\n",
    "    df_all[\"log_cpy\"], n_bins=17\n",
    ")  # The n_bins=17 is to ensure we don't accidentally flip axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_histplot(data, x, y, normed=True, *args, **kwargs):\n",
    "\n",
    "    # Get the appropriate facets\n",
    "    vectorizer = data[\"vectorizer\"].unique()[0]\n",
    "    field = data[\"fields_of_study_0\"].unique()[0]\n",
    "\n",
    "    density_bins = get_bins(data[x])\n",
    "\n",
    "    if normed:\n",
    "        hist2d, _, _ = np.histogram2d(data[x], data[y], bins=(density_bins, cpy_bins))\n",
    "        density_hist, _ = np.histogram(data[x], bins=density_bins)\n",
    "        hist2d_normed = hist2d / density_hist.reshape(1, -1).T\n",
    "\n",
    "        # Plot the data\n",
    "        plt.pcolormesh(\n",
    "            density_bins,\n",
    "            cpy_bins,\n",
    "            hist2d_normed.T,\n",
    "            shading='auto',\n",
    "            cmap=\"viridis\",\n",
    "        )\n",
    "    else:\n",
    "        sns.histplot(data, x=x, y=y, bins=(density_bins, cpy_bins), *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = sns.FacetGrid(data=df_all, row=\"vectorizer\", col=\"fields_of_study_0\", sharex=False)\n",
    "fg.map_dataframe(custom_histplot, x=\"density\", y=\"log_cpy\", normed=True)\n",
    "fg.set_titles(\"{row_name} | {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focused Normalized Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.query(\"vectorizer in ['SciBERT', 'Word2Vec']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = sns.FacetGrid(data=df, row=\"vectorizer\", col=\"fields_of_study_0\", sharex=False)\n",
    "fg.map_dataframe(custom_histplot, x=\"density\", y=\"log_cpy\", normed=True)\n",
    "fg.set_titles(\"{row_name} | {col_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Modeling\n",
    "\n",
    "Note to future self: cmdstanpy and cmdstan advertise conda as *the* way to install.\n",
    "Don't listen to them.\n",
    "I couldn't get it to compile when I used conda for anything cmdstanpy related.\n",
    "I downloaded the repo and compiled manually, and used pip for cmdstanpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cmdstanpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to wide format so we can use the same train_test division\n",
    "df_eval = df_all.pivot_table(index=\"identifier\", columns=\"vectorizer\", values=\"density\")\n",
    "log_cpy = df_all.pivot_table(index=\"identifier\", columns=\"vectorizer\", values=\"log_cpy\").iloc[:, 0]\n",
    "df_eval[\"log_cpy\"] = log_cpy\n",
    "df_eval.columns.name = None\n",
    "df_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample\n",
    "if config[\"eval_sample_size\"] is not None:\n",
    "    df_eval = df_eval.sample(config[\"eval_sample_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "df_train, df_test = train_test_split(df_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global fits\n",
    "$(c \\sim \\rho_t)$ vs $(c \\sim \\rho_v)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = \"Word2Vec\"\n",
    "\n",
    "# Drop na\n",
    "df_train_i = df_train.dropna(subset=[vectorizer, \"log_cpy\"])\n",
    "df_test_i = df_train.dropna(subset=[vectorizer, \"log_cpy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"N\": len(df_train_i),\n",
    "    \"x\": df_train_i[vectorizer].values,\n",
    "    \"y\": df_train_i[\"log_cpy\"].values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cmdstanpy.CmdStanModel(stan_file=\"../stan_models/linear_regression.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model.sample(\n",
    "    data=data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draws = fit.draws_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.special.logsumexp(draws[\"log_p\"]) - np.log(len(draws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"N\": len(df_test_i),\n",
    "    \"x\": df_test_i[vectorizer].values,\n",
    "    \"y\": df_test_i[\"log_cpy\"].values,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show parameters\n",
    "pg = sns.PairGrid(data=fit.draws_pd(), vars=[\"alpha\", \"beta\", \"sigma\"], diag_sharey=False)\n",
    "pg.map_lower(sns.histplot)\n",
    "pg.map_diag(sns.kdeplot)\n",
    "pg.map_upper(sns.scatterplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior Predictive Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ppc = cmdstanpy.CmdStanModel(stan_file='../stan_models/linear_regression_ppc.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_quantities = model_ppc.generate_quantities(data=data, previous_fit=fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = new_quantities.draws_pd(inc_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rep_cols = [col for col in samples.columns if 'y_rep' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = samples[y_rep_cols].mean(axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = means - data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(\n",
    "    samples[y_rep_cols].values.flatten(),\n",
    ")\n",
    "sns.histplot(\n",
    "    data[\"y\"],\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    data=diffs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citesim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
